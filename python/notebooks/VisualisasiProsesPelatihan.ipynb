{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fcdc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data: E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Surabaya.xlsx\n",
      "✓ Jumlah baris asli (raw): 1043\n",
      "✓ Sampel window: 1036 (window=7)\n",
      "✓ Data ternormalisasi (0-1).\n",
      "[Info] Total: 1036 | Train: 828 | Val: 83 | Test: 208\n",
      "   -> Selesai epoch : 114\n",
      "   -> Best epoch    : 114 (val_loss=0.000222)\n",
      "   -> Last loss     : 0.000576 | Last val_loss : 0.000222\n",
      "   -> MAPE          : 2.6390%\n",
      "   ✓ Model disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Surabaya_w7_e114_b64\\Surabaya_model_W7_E114_B64_DO0.02.h5\n",
      "   ✓ Scaler disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Surabaya_w7_e114_b64\\Surabaya_scaler_W7_E114_B64_DO0.02.pkl\n",
      "   ✓ Metadata disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Surabaya_w7_e114_b64\\Surabaya_metadata.json\n",
      "   ✓ Grafik Loss REVISI disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Surabaya_w7_e114_b64\\Surabaya_Loss_Graph_Revisi.png\n",
      "   ✓ Plot disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Surabaya_w7_e114_b64\\Surabaya_Plot.png\n",
      "   ✓ Prediksi disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Surabaya_w7_e114_b64\\Surabaya_Prediksi_W7_E114_B64.xlsx\n",
      "   ✓ Ringkasan disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Surabaya_w7_e114_b64\\Surabaya_Summary_W7_E114_B64.xlsx\n",
      "\n",
      "Selesai untuk file ini.\n"
     ]
    }
   ],
   "source": [
    "#VISUALISASI LOSS DATA (REVISI FINAL)\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(42)\n",
    "    python_random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "reset_seeds()\n",
    "\n",
    "\n",
    "FIXED_WINDOW_SIZE = 7\n",
    "FIXED_EPOCH = 114\n",
    "FIXED_BATCH_SIZE = 64\n",
    "VAL_RATIO = 0.1\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "path_file_input = r\"E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Surabaya.xlsx\"\n",
    "path_output_folder = r\"E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\"\n",
    "os.makedirs(path_output_folder, exist_ok=True)\n",
    "\n",
    "def create_sliding_window(dataset, window_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        X.append(dataset[i:(i + window_size), 0])\n",
    "        Y.append(dataset[i + window_size, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def build_model_lstm(input_shape):\n",
    "    tf.random.set_seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "if not os.path.exists(path_file_input):\n",
    "    raise FileNotFoundError(f\"File tidak ditemukan: {path_file_input}\")\n",
    "\n",
    "df = pd.read_excel(path_file_input)\n",
    "nama_kolom_tanggal = 'Tanggal' \n",
    "\n",
    "if 'Harga (Rp)' not in df.columns:\n",
    "    raise KeyError(f\"Kolom 'Harga (Rp)' tidak ditemukan di file {path_file_input}\")\n",
    "\n",
    "df[nama_kolom_tanggal] = pd.to_datetime(df[nama_kolom_tanggal])\n",
    "raw_data = df['Harga (Rp)'].values.reshape(-1, 1)\n",
    "print(f\"✓ Data: {path_file_input}\")\n",
    "print(f\"✓ Jumlah baris asli (raw): {len(raw_data)}\")\n",
    "\n",
    "region_name = os.path.basename(path_file_input).replace(\"Data_Clean_\", \"\").replace(\".xlsx\", \"\")\n",
    "\n",
    "X_full_raw, Y_full_raw = create_sliding_window(raw_data, FIXED_WINDOW_SIZE)\n",
    "print(f\"✓ Sampel window: {len(X_full_raw)} (window={FIXED_WINDOW_SIZE})\")\n",
    "train_size = int(len(X_full_raw) * 0.8)\n",
    "y_test_orig = Y_full_raw[train_size:]  \n",
    "\n",
    "\n",
    "raw_train_segment = raw_data[:train_size + FIXED_WINDOW_SIZE]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(raw_train_segment)\n",
    "scaled_data = scaler.transform(raw_data)\n",
    "print(\"✓ Data ternormalisasi (0-1).\")\n",
    "\n",
    "\n",
    "X_full_scaled, Y_full_scaled = create_sliding_window(scaled_data, FIXED_WINDOW_SIZE)\n",
    "X_full_scaled = X_full_scaled.reshape(X_full_scaled.shape[0], X_full_scaled.shape[1], 1)\n",
    "\n",
    "X_train_all, X_test = X_full_scaled[:train_size], X_full_scaled[train_size:]\n",
    "y_train_all = Y_full_scaled[:train_size]\n",
    "\n",
    "# Validasi tail dari TRAIN\n",
    "val_cut = int(len(X_train_all) * (1 - VAL_RATIO))\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "print(f\"[Info] Total: {len(X_full_scaled)} | Train: {len(X_train_all)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "\n",
    "reset_seeds()\n",
    "model = build_model_lstm((X_train.shape[1], 1))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=FIXED_EPOCH,\n",
    "    batch_size=FIXED_BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss', None)\n",
    "last_loss = float(train_loss[-1])\n",
    "last_val_loss = float(val_loss[-1]) if val_loss is not None else None\n",
    "best_epoch = int(np.argmin(val_loss) + 1) if val_loss is not None else int(np.argmin(train_loss) + 1)\n",
    "best_val = float(np.min(val_loss)) if val_loss is not None else float(np.min(train_loss))\n",
    "\n",
    "print(f\"   -> Selesai epoch : {FIXED_EPOCH}\")\n",
    "print(f\"   -> Best epoch    : {best_epoch} (val_loss={best_val:.6f})\")\n",
    "print(f\"   -> Last loss     : {last_loss:.6f}\" + (f\" | Last val_loss : {last_val_loss:.6f}\" if last_val_loss is not None else \"\"))\n",
    "\n",
    "\n",
    "predictions_scaled = model.predict(X_test, verbose=0)\n",
    "predictions_real = scaler.inverse_transform(predictions_scaled)\n",
    "mape = mean_absolute_percentage_error(y_test_orig.ravel(), predictions_real.ravel()) * 100\n",
    "print(f\"   -> MAPE          : {mape:.4f}%\")\n",
    "\n",
    "\n",
    "region_out_dir = os.path.join(path_output_folder, f\"{region_name}_w{FIXED_WINDOW_SIZE}_e{FIXED_EPOCH}_b{FIXED_BATCH_SIZE}\")\n",
    "os.makedirs(region_out_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(region_out_dir, f\"{region_name}_model_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.h5\")\n",
    "scaler_path = os.path.join(region_out_dir, f\"{region_name}_scaler_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.pkl\")\n",
    "meta_path = os.path.join(region_out_dir, f\"{region_name}_metadata.json\")\n",
    "\n",
    "model.save(model_path)\n",
    "print(f\"   ✓ Model disimpan: {model_path}\")\n",
    "\n",
    "# Simpan scaler\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"   ✓ Scaler disimpan: {scaler_path}\")\n",
    "\n",
    "metadata = {\n",
    "    \"region\": region_name,\n",
    "    \"window_size\": int(FIXED_WINDOW_SIZE),\n",
    "    \"epoch\": int(FIXED_EPOCH),\n",
    "    \"batch_size\": int(FIXED_BATCH_SIZE),\n",
    "    \"dropout_rate\": float(DROPOUT_RATE),\n",
    "    \"learning_rate\": float(LEARNING_RATE),\n",
    "    \"val_ratio\": float(VAL_RATIO),\n",
    "    \"train_samples\": int(len(X_train_all)),\n",
    "    \"val_samples\": int(len(X_val)),\n",
    "    \"test_samples\": int(len(X_test)),\n",
    "    \"best_epoch_val\": int(best_epoch),\n",
    "    \"best_val_loss\": float(best_val),\n",
    "    \"last_loss\": float(last_loss),\n",
    "    \"last_val_loss\": float(last_val_loss) if last_val_loss is not None else None,\n",
    "    \"mape_test_percent\": float(mape)\n",
    "}\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"   ✓ Metadata disimpan: {meta_path}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "\n",
    "# Plot Garis\n",
    "plt.plot(history.history['loss'], label='Training Loss (Data Latih)', linewidth=2)\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss (Data Validasi)', linewidth=2)\n",
    "\n",
    "plt.xlim(0, FIXED_EPOCH)\n",
    "ticks = list(range(0, FIXED_EPOCH, 20)) + [FIXED_EPOCH]\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.title(f'Grafik Penurunan Error (Loss) | {region_name}\\n(Semakin turun mendekati 0 = Semakin Akurat)', fontsize=14)\n",
    "plt.ylabel('Tingkat Error (MSE - Skala Normalisasi 0-1)', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Simpan\n",
    "loss_plot_path = os.path.join(region_out_dir, f\"{region_name}_Loss_Graph_Revisi.png\")\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Grafik Loss REVISI disimpan: {loss_plot_path}\")\n",
    "\n",
    "\n",
    "all_dates = df[nama_kolom_tanggal].values[FIXED_WINDOW_SIZE:]\n",
    "\n",
    "# Ambil tanggal khusus untuk bagian TEST saja\n",
    "test_dates = all_dates[train_size:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "plt.plot(test_dates, y_test_orig, label='Actual (Real Data)')\n",
    "plt.plot(test_dates, predictions_real, label='Predicted (Denormalized)')\n",
    "\n",
    "plt.title(f'{region_name} | W{FIXED_WINDOW_SIZE} E{FIXED_EPOCH} B{FIXED_BATCH_SIZE} DO{DROPOUT_RATE}')\n",
    "plt.xlabel('Tanggal dan Tahun')\n",
    "plt.ylabel('Harga (Rp)')\n",
    "\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) \n",
    "plt.gcf().autofmt_xdate() \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plot_path = os.path.join(region_out_dir, f\"{region_name}_Plot.png\")\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Plot disimpan: {plot_path}\")\n",
    "\n",
    "y_test_safe = np.where(y_test_orig.flatten() == 0, np.finfo(float).eps, y_test_orig.flatten())\n",
    "err_pct = np.abs((y_test_orig.flatten() - predictions_real.flatten()) / y_test_safe) * 100\n",
    "out_df = pd.DataFrame({\n",
    "    'Tanggal': test_dates,  \n",
    "    'Actual (Real)': y_test_orig.flatten(),\n",
    "    'Predicted (Real)': predictions_real.flatten(),\n",
    "    'Selisih': (y_test_orig.flatten() - predictions_real.flatten()),\n",
    "    'Error (%)': err_pct\n",
    "})\n",
    "excel_path = os.path.join(region_out_dir, f\"{region_name}_Prediksi_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "out_df.to_excel(excel_path, index=False)\n",
    "print(f\"   ✓ Prediksi disimpan: {excel_path}\")\n",
    "\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'Region': region_name,\n",
    "    'Window Size': int(FIXED_WINDOW_SIZE),\n",
    "    'Epoch': int(FIXED_EPOCH),\n",
    "    'Batch Size': int(FIXED_BATCH_SIZE),\n",
    "    'Dropout': float(DROPOUT_RATE),\n",
    "    'Best Epoch (val)': int(best_epoch),\n",
    "    'Best Val Loss': float(best_val),\n",
    "    'Last Loss': float(last_loss),\n",
    "    'Last Val Loss': float(last_val_loss) if last_val_loss is not None else None,\n",
    "    'MAPE (%)': float(mape)\n",
    "}])\n",
    "summary_path = os.path.join(region_out_dir, f\"{region_name}_Summary_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "summary.to_excel(summary_path, index=False)\n",
    "print(f\"   ✓ Ringkasan disimpan: {summary_path}\")\n",
    "\n",
    "print(\"\\nSelesai untuk file ini.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebd5965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data: E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Banyuwangi.xlsx\n",
      "✓ Jumlah baris asli (raw): 1043\n",
      "✓ Sampel window: 1036 (window=7)\n",
      "✓ Data ternormalisasi (0-1).\n",
      "[Info] Total: 1036 | Train: 828 | Val: 83 | Test: 208\n",
      "   -> Selesai epoch : 114\n",
      "   -> Best epoch    : 112 (val_loss=0.000267)\n",
      "   -> Last loss     : 0.001090 | Last val_loss : 0.000321\n",
      "   -> MAPE          : 4.1406%\n",
      "   ✓ Model disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Banyuwangi_w7_e114_b64\\Banyuwangi_model_W7_E114_B64_DO0.02.h5\n",
      "   ✓ Scaler disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Banyuwangi_w7_e114_b64\\Banyuwangi_scaler_W7_E114_B64_DO0.02.pkl\n",
      "   ✓ Metadata disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Banyuwangi_w7_e114_b64\\Banyuwangi_metadata.json\n",
      "   ✓ Grafik Loss REVISI disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Banyuwangi_w7_e114_b64\\Banyuwangi_Loss_Graph_Revisi.png\n",
      "   ✓ Plot disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Banyuwangi_w7_e114_b64\\Banyuwangi_Plot.png\n",
      "   ✓ Prediksi disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Banyuwangi_w7_e114_b64\\Banyuwangi_Prediksi_W7_E114_B64.xlsx\n",
      "   ✓ Ringkasan disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Banyuwangi_w7_e114_b64\\Banyuwangi_Summary_W7_E114_B64.xlsx\n",
      "\n",
      "Selesai untuk file ini.\n"
     ]
    }
   ],
   "source": [
    "#VISUALISASI LOSS DATA (REVISI FINAL)\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(42)\n",
    "    python_random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "reset_seeds()\n",
    "\n",
    "\n",
    "FIXED_WINDOW_SIZE = 7\n",
    "FIXED_EPOCH = 114\n",
    "FIXED_BATCH_SIZE = 64\n",
    "VAL_RATIO = 0.1\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "path_file_input = r\"E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Banyuwangi.xlsx\"\n",
    "path_output_folder = r\"E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\"\n",
    "os.makedirs(path_output_folder, exist_ok=True)\n",
    "\n",
    "def create_sliding_window(dataset, window_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        X.append(dataset[i:(i + window_size), 0])\n",
    "        Y.append(dataset[i + window_size, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def build_model_lstm(input_shape):\n",
    "    tf.random.set_seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "if not os.path.exists(path_file_input):\n",
    "    raise FileNotFoundError(f\"File tidak ditemukan: {path_file_input}\")\n",
    "\n",
    "df = pd.read_excel(path_file_input)\n",
    "nama_kolom_tanggal = 'Tanggal' \n",
    "\n",
    "if 'Harga (Rp)' not in df.columns:\n",
    "    raise KeyError(f\"Kolom 'Harga (Rp)' tidak ditemukan di file {path_file_input}\")\n",
    "\n",
    "df[nama_kolom_tanggal] = pd.to_datetime(df[nama_kolom_tanggal])\n",
    "raw_data = df['Harga (Rp)'].values.reshape(-1, 1)\n",
    "print(f\"✓ Data: {path_file_input}\")\n",
    "print(f\"✓ Jumlah baris asli (raw): {len(raw_data)}\")\n",
    "\n",
    "region_name = os.path.basename(path_file_input).replace(\"Data_Clean_\", \"\").replace(\".xlsx\", \"\")\n",
    "\n",
    "X_full_raw, Y_full_raw = create_sliding_window(raw_data, FIXED_WINDOW_SIZE)\n",
    "print(f\"✓ Sampel window: {len(X_full_raw)} (window={FIXED_WINDOW_SIZE})\")\n",
    "train_size = int(len(X_full_raw) * 0.8)\n",
    "y_test_orig = Y_full_raw[train_size:]  \n",
    "\n",
    "\n",
    "raw_train_segment = raw_data[:train_size + FIXED_WINDOW_SIZE]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(raw_train_segment)\n",
    "scaled_data = scaler.transform(raw_data)\n",
    "print(\"✓ Data ternormalisasi (0-1).\")\n",
    "\n",
    "\n",
    "X_full_scaled, Y_full_scaled = create_sliding_window(scaled_data, FIXED_WINDOW_SIZE)\n",
    "X_full_scaled = X_full_scaled.reshape(X_full_scaled.shape[0], X_full_scaled.shape[1], 1)\n",
    "\n",
    "X_train_all, X_test = X_full_scaled[:train_size], X_full_scaled[train_size:]\n",
    "y_train_all = Y_full_scaled[:train_size]\n",
    "\n",
    "# Validasi tail dari TRAIN\n",
    "val_cut = int(len(X_train_all) * (1 - VAL_RATIO))\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "print(f\"[Info] Total: {len(X_full_scaled)} | Train: {len(X_train_all)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "\n",
    "reset_seeds()\n",
    "model = build_model_lstm((X_train.shape[1], 1))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=FIXED_EPOCH,\n",
    "    batch_size=FIXED_BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss', None)\n",
    "last_loss = float(train_loss[-1])\n",
    "last_val_loss = float(val_loss[-1]) if val_loss is not None else None\n",
    "best_epoch = int(np.argmin(val_loss) + 1) if val_loss is not None else int(np.argmin(train_loss) + 1)\n",
    "best_val = float(np.min(val_loss)) if val_loss is not None else float(np.min(train_loss))\n",
    "\n",
    "print(f\"   -> Selesai epoch : {FIXED_EPOCH}\")\n",
    "print(f\"   -> Best epoch    : {best_epoch} (val_loss={best_val:.6f})\")\n",
    "print(f\"   -> Last loss     : {last_loss:.6f}\" + (f\" | Last val_loss : {last_val_loss:.6f}\" if last_val_loss is not None else \"\"))\n",
    "\n",
    "\n",
    "predictions_scaled = model.predict(X_test, verbose=0)\n",
    "predictions_real = scaler.inverse_transform(predictions_scaled)\n",
    "mape = mean_absolute_percentage_error(y_test_orig.ravel(), predictions_real.ravel()) * 100\n",
    "print(f\"   -> MAPE          : {mape:.4f}%\")\n",
    "\n",
    "\n",
    "region_out_dir = os.path.join(path_output_folder, f\"{region_name}_w{FIXED_WINDOW_SIZE}_e{FIXED_EPOCH}_b{FIXED_BATCH_SIZE}\")\n",
    "os.makedirs(region_out_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(region_out_dir, f\"{region_name}_model_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.h5\")\n",
    "scaler_path = os.path.join(region_out_dir, f\"{region_name}_scaler_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.pkl\")\n",
    "meta_path = os.path.join(region_out_dir, f\"{region_name}_metadata.json\")\n",
    "\n",
    "model.save(model_path)\n",
    "print(f\"   ✓ Model disimpan: {model_path}\")\n",
    "\n",
    "# Simpan scaler\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"   ✓ Scaler disimpan: {scaler_path}\")\n",
    "\n",
    "metadata = {\n",
    "    \"region\": region_name,\n",
    "    \"window_size\": int(FIXED_WINDOW_SIZE),\n",
    "    \"epoch\": int(FIXED_EPOCH),\n",
    "    \"batch_size\": int(FIXED_BATCH_SIZE),\n",
    "    \"dropout_rate\": float(DROPOUT_RATE),\n",
    "    \"learning_rate\": float(LEARNING_RATE),\n",
    "    \"val_ratio\": float(VAL_RATIO),\n",
    "    \"train_samples\": int(len(X_train_all)),\n",
    "    \"val_samples\": int(len(X_val)),\n",
    "    \"test_samples\": int(len(X_test)),\n",
    "    \"best_epoch_val\": int(best_epoch),\n",
    "    \"best_val_loss\": float(best_val),\n",
    "    \"last_loss\": float(last_loss),\n",
    "    \"last_val_loss\": float(last_val_loss) if last_val_loss is not None else None,\n",
    "    \"mape_test_percent\": float(mape)\n",
    "}\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"   ✓ Metadata disimpan: {meta_path}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "\n",
    "# Plot Garis\n",
    "plt.plot(history.history['loss'], label='Training Loss (Data Latih)', linewidth=2)\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss (Data Validasi)', linewidth=2)\n",
    "\n",
    "plt.xlim(0, FIXED_EPOCH)\n",
    "ticks = list(range(0, FIXED_EPOCH, 20)) + [FIXED_EPOCH]\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.title(f'Grafik Penurunan Error (Loss) | {region_name}\\n(Semakin turun mendekati 0 = Semakin Akurat)', fontsize=14)\n",
    "plt.ylabel('Tingkat Error (MSE - Skala Normalisasi 0-1)', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Simpan\n",
    "loss_plot_path = os.path.join(region_out_dir, f\"{region_name}_Loss_Graph_Revisi.png\")\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Grafik Loss REVISI disimpan: {loss_plot_path}\")\n",
    "\n",
    "\n",
    "all_dates = df[nama_kolom_tanggal].values[FIXED_WINDOW_SIZE:]\n",
    "\n",
    "# Ambil tanggal khusus untuk bagian TEST saja\n",
    "test_dates = all_dates[train_size:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "plt.plot(test_dates, y_test_orig, label='Actual (Real Data)')\n",
    "plt.plot(test_dates, predictions_real, label='Predicted (Denormalized)')\n",
    "\n",
    "plt.title(f'{region_name} | W{FIXED_WINDOW_SIZE} E{FIXED_EPOCH} B{FIXED_BATCH_SIZE} DO{DROPOUT_RATE}')\n",
    "plt.xlabel('Tanggal dan Tahun')\n",
    "plt.ylabel('Harga (Rp)')\n",
    "\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) \n",
    "plt.gcf().autofmt_xdate() \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plot_path = os.path.join(region_out_dir, f\"{region_name}_Plot.png\")\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Plot disimpan: {plot_path}\")\n",
    "\n",
    "y_test_safe = np.where(y_test_orig.flatten() == 0, np.finfo(float).eps, y_test_orig.flatten())\n",
    "err_pct = np.abs((y_test_orig.flatten() - predictions_real.flatten()) / y_test_safe) * 100\n",
    "out_df = pd.DataFrame({\n",
    "    'Tanggal': test_dates,  \n",
    "    'Actual (Real)': y_test_orig.flatten(),\n",
    "    'Predicted (Real)': predictions_real.flatten(),\n",
    "    'Selisih': (y_test_orig.flatten() - predictions_real.flatten()),\n",
    "    'Error (%)': err_pct\n",
    "})\n",
    "excel_path = os.path.join(region_out_dir, f\"{region_name}_Prediksi_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "out_df.to_excel(excel_path, index=False)\n",
    "print(f\"   ✓ Prediksi disimpan: {excel_path}\")\n",
    "\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'Region': region_name,\n",
    "    'Window Size': int(FIXED_WINDOW_SIZE),\n",
    "    'Epoch': int(FIXED_EPOCH),\n",
    "    'Batch Size': int(FIXED_BATCH_SIZE),\n",
    "    'Dropout': float(DROPOUT_RATE),\n",
    "    'Best Epoch (val)': int(best_epoch),\n",
    "    'Best Val Loss': float(best_val),\n",
    "    'Last Loss': float(last_loss),\n",
    "    'Last Val Loss': float(last_val_loss) if last_val_loss is not None else None,\n",
    "    'MAPE (%)': float(mape)\n",
    "}])\n",
    "summary_path = os.path.join(region_out_dir, f\"{region_name}_Summary_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "summary.to_excel(summary_path, index=False)\n",
    "print(f\"   ✓ Ringkasan disimpan: {summary_path}\")\n",
    "\n",
    "print(\"\\nSelesai untuk file ini.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66991f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data: E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Blitar.xlsx\n",
      "✓ Jumlah baris asli (raw): 1043\n",
      "✓ Sampel window: 1036 (window=7)\n",
      "✓ Data ternormalisasi (0-1).\n",
      "[Info] Total: 1036 | Train: 828 | Val: 83 | Test: 208\n",
      "   -> Selesai epoch : 114\n",
      "   -> Best epoch    : 114 (val_loss=0.000583)\n",
      "   -> Last loss     : 0.000890 | Last val_loss : 0.000583\n",
      "   -> MAPE          : 3.4628%\n",
      "   ✓ Model disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Blitar_w7_e114_b64\\Blitar_model_W7_E114_B64_DO0.02.h5\n",
      "   ✓ Scaler disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Blitar_w7_e114_b64\\Blitar_scaler_W7_E114_B64_DO0.02.pkl\n",
      "   ✓ Metadata disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Blitar_w7_e114_b64\\Blitar_metadata.json\n",
      "   ✓ Grafik Loss REVISI disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Blitar_w7_e114_b64\\Blitar_Loss_Graph_Revisi.png\n",
      "   ✓ Plot disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Blitar_w7_e114_b64\\Blitar_Plot.png\n",
      "   ✓ Prediksi disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Blitar_w7_e114_b64\\Blitar_Prediksi_W7_E114_B64.xlsx\n",
      "   ✓ Ringkasan disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Blitar_w7_e114_b64\\Blitar_Summary_W7_E114_B64.xlsx\n",
      "\n",
      "Selesai untuk file ini.\n"
     ]
    }
   ],
   "source": [
    "#VISUALISASI LOSS DATA (REVISI FINAL)\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(42)\n",
    "    python_random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "reset_seeds()\n",
    "\n",
    "\n",
    "FIXED_WINDOW_SIZE = 7\n",
    "FIXED_EPOCH = 114\n",
    "FIXED_BATCH_SIZE = 64\n",
    "VAL_RATIO = 0.1\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "path_file_input = r\"E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Blitar.xlsx\"\n",
    "path_output_folder = r\"E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\"\n",
    "os.makedirs(path_output_folder, exist_ok=True)\n",
    "\n",
    "def create_sliding_window(dataset, window_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        X.append(dataset[i:(i + window_size), 0])\n",
    "        Y.append(dataset[i + window_size, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def build_model_lstm(input_shape):\n",
    "    tf.random.set_seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "if not os.path.exists(path_file_input):\n",
    "    raise FileNotFoundError(f\"File tidak ditemukan: {path_file_input}\")\n",
    "\n",
    "df = pd.read_excel(path_file_input)\n",
    "nama_kolom_tanggal = 'Tanggal' \n",
    "\n",
    "if 'Harga (Rp)' not in df.columns:\n",
    "    raise KeyError(f\"Kolom 'Harga (Rp)' tidak ditemukan di file {path_file_input}\")\n",
    "\n",
    "df[nama_kolom_tanggal] = pd.to_datetime(df[nama_kolom_tanggal])\n",
    "raw_data = df['Harga (Rp)'].values.reshape(-1, 1)\n",
    "print(f\"✓ Data: {path_file_input}\")\n",
    "print(f\"✓ Jumlah baris asli (raw): {len(raw_data)}\")\n",
    "\n",
    "region_name = os.path.basename(path_file_input).replace(\"Data_Clean_\", \"\").replace(\".xlsx\", \"\")\n",
    "\n",
    "X_full_raw, Y_full_raw = create_sliding_window(raw_data, FIXED_WINDOW_SIZE)\n",
    "print(f\"✓ Sampel window: {len(X_full_raw)} (window={FIXED_WINDOW_SIZE})\")\n",
    "train_size = int(len(X_full_raw) * 0.8)\n",
    "y_test_orig = Y_full_raw[train_size:]  \n",
    "\n",
    "\n",
    "raw_train_segment = raw_data[:train_size + FIXED_WINDOW_SIZE]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(raw_train_segment)\n",
    "scaled_data = scaler.transform(raw_data)\n",
    "print(\"✓ Data ternormalisasi (0-1).\")\n",
    "\n",
    "\n",
    "X_full_scaled, Y_full_scaled = create_sliding_window(scaled_data, FIXED_WINDOW_SIZE)\n",
    "X_full_scaled = X_full_scaled.reshape(X_full_scaled.shape[0], X_full_scaled.shape[1], 1)\n",
    "\n",
    "X_train_all, X_test = X_full_scaled[:train_size], X_full_scaled[train_size:]\n",
    "y_train_all = Y_full_scaled[:train_size]\n",
    "\n",
    "# Validasi tail dari TRAIN\n",
    "val_cut = int(len(X_train_all) * (1 - VAL_RATIO))\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "print(f\"[Info] Total: {len(X_full_scaled)} | Train: {len(X_train_all)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "\n",
    "reset_seeds()\n",
    "model = build_model_lstm((X_train.shape[1], 1))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=FIXED_EPOCH,\n",
    "    batch_size=FIXED_BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss', None)\n",
    "last_loss = float(train_loss[-1])\n",
    "last_val_loss = float(val_loss[-1]) if val_loss is not None else None\n",
    "best_epoch = int(np.argmin(val_loss) + 1) if val_loss is not None else int(np.argmin(train_loss) + 1)\n",
    "best_val = float(np.min(val_loss)) if val_loss is not None else float(np.min(train_loss))\n",
    "\n",
    "print(f\"   -> Selesai epoch : {FIXED_EPOCH}\")\n",
    "print(f\"   -> Best epoch    : {best_epoch} (val_loss={best_val:.6f})\")\n",
    "print(f\"   -> Last loss     : {last_loss:.6f}\" + (f\" | Last val_loss : {last_val_loss:.6f}\" if last_val_loss is not None else \"\"))\n",
    "\n",
    "\n",
    "predictions_scaled = model.predict(X_test, verbose=0)\n",
    "predictions_real = scaler.inverse_transform(predictions_scaled)\n",
    "mape = mean_absolute_percentage_error(y_test_orig.ravel(), predictions_real.ravel()) * 100\n",
    "print(f\"   -> MAPE          : {mape:.4f}%\")\n",
    "\n",
    "\n",
    "region_out_dir = os.path.join(path_output_folder, f\"{region_name}_w{FIXED_WINDOW_SIZE}_e{FIXED_EPOCH}_b{FIXED_BATCH_SIZE}\")\n",
    "os.makedirs(region_out_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(region_out_dir, f\"{region_name}_model_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.h5\")\n",
    "scaler_path = os.path.join(region_out_dir, f\"{region_name}_scaler_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.pkl\")\n",
    "meta_path = os.path.join(region_out_dir, f\"{region_name}_metadata.json\")\n",
    "\n",
    "model.save(model_path)\n",
    "print(f\"   ✓ Model disimpan: {model_path}\")\n",
    "\n",
    "# Simpan scaler\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"   ✓ Scaler disimpan: {scaler_path}\")\n",
    "\n",
    "metadata = {\n",
    "    \"region\": region_name,\n",
    "    \"window_size\": int(FIXED_WINDOW_SIZE),\n",
    "    \"epoch\": int(FIXED_EPOCH),\n",
    "    \"batch_size\": int(FIXED_BATCH_SIZE),\n",
    "    \"dropout_rate\": float(DROPOUT_RATE),\n",
    "    \"learning_rate\": float(LEARNING_RATE),\n",
    "    \"val_ratio\": float(VAL_RATIO),\n",
    "    \"train_samples\": int(len(X_train_all)),\n",
    "    \"val_samples\": int(len(X_val)),\n",
    "    \"test_samples\": int(len(X_test)),\n",
    "    \"best_epoch_val\": int(best_epoch),\n",
    "    \"best_val_loss\": float(best_val),\n",
    "    \"last_loss\": float(last_loss),\n",
    "    \"last_val_loss\": float(last_val_loss) if last_val_loss is not None else None,\n",
    "    \"mape_test_percent\": float(mape)\n",
    "}\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"   ✓ Metadata disimpan: {meta_path}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "\n",
    "# Plot Garis\n",
    "plt.plot(history.history['loss'], label='Training Loss (Data Latih)', linewidth=2)\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss (Data Validasi)', linewidth=2)\n",
    "\n",
    "plt.xlim(0, FIXED_EPOCH)\n",
    "ticks = list(range(0, FIXED_EPOCH, 20)) + [FIXED_EPOCH]\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.title(f'Grafik Penurunan Error (Loss) | {region_name}\\n(Semakin turun mendekati 0 = Semakin Akurat)', fontsize=14)\n",
    "plt.ylabel('Tingkat Error (MSE - Skala Normalisasi 0-1)', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Simpan\n",
    "loss_plot_path = os.path.join(region_out_dir, f\"{region_name}_Loss_Graph_Revisi.png\")\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Grafik Loss REVISI disimpan: {loss_plot_path}\")\n",
    "\n",
    "\n",
    "all_dates = df[nama_kolom_tanggal].values[FIXED_WINDOW_SIZE:]\n",
    "\n",
    "# Ambil tanggal khusus untuk bagian TEST saja\n",
    "test_dates = all_dates[train_size:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "plt.plot(test_dates, y_test_orig, label='Actual (Real Data)')\n",
    "plt.plot(test_dates, predictions_real, label='Predicted (Denormalized)')\n",
    "\n",
    "plt.title(f'{region_name} | W{FIXED_WINDOW_SIZE} E{FIXED_EPOCH} B{FIXED_BATCH_SIZE} DO{DROPOUT_RATE}')\n",
    "plt.xlabel('Tanggal dan Tahun')\n",
    "plt.ylabel('Harga (Rp)')\n",
    "\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) \n",
    "plt.gcf().autofmt_xdate() \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plot_path = os.path.join(region_out_dir, f\"{region_name}_Plot.png\")\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Plot disimpan: {plot_path}\")\n",
    "\n",
    "y_test_safe = np.where(y_test_orig.flatten() == 0, np.finfo(float).eps, y_test_orig.flatten())\n",
    "err_pct = np.abs((y_test_orig.flatten() - predictions_real.flatten()) / y_test_safe) * 100\n",
    "out_df = pd.DataFrame({\n",
    "    'Tanggal': test_dates,  \n",
    "    'Actual (Real)': y_test_orig.flatten(),\n",
    "    'Predicted (Real)': predictions_real.flatten(),\n",
    "    'Selisih': (y_test_orig.flatten() - predictions_real.flatten()),\n",
    "    'Error (%)': err_pct\n",
    "})\n",
    "excel_path = os.path.join(region_out_dir, f\"{region_name}_Prediksi_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "out_df.to_excel(excel_path, index=False)\n",
    "print(f\"   ✓ Prediksi disimpan: {excel_path}\")\n",
    "\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'Region': region_name,\n",
    "    'Window Size': int(FIXED_WINDOW_SIZE),\n",
    "    'Epoch': int(FIXED_EPOCH),\n",
    "    'Batch Size': int(FIXED_BATCH_SIZE),\n",
    "    'Dropout': float(DROPOUT_RATE),\n",
    "    'Best Epoch (val)': int(best_epoch),\n",
    "    'Best Val Loss': float(best_val),\n",
    "    'Last Loss': float(last_loss),\n",
    "    'Last Val Loss': float(last_val_loss) if last_val_loss is not None else None,\n",
    "    'MAPE (%)': float(mape)\n",
    "}])\n",
    "summary_path = os.path.join(region_out_dir, f\"{region_name}_Summary_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "summary.to_excel(summary_path, index=False)\n",
    "print(f\"   ✓ Ringkasan disimpan: {summary_path}\")\n",
    "\n",
    "print(\"\\nSelesai untuk file ini.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2daaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data: E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_jember.xlsx\n",
      "✓ Jumlah baris asli (raw): 1043\n",
      "✓ Sampel window: 1036 (window=7)\n",
      "✓ Data ternormalisasi (0-1).\n",
      "[Info] Total: 1036 | Train: 828 | Val: 83 | Test: 208\n",
      "   -> Selesai epoch : 114\n",
      "   -> Best epoch    : 114 (val_loss=0.000313)\n",
      "   -> Last loss     : 0.000446 | Last val_loss : 0.000313\n",
      "   -> MAPE          : 3.9588%\n",
      "   ✓ Model disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\jember_w7_e114_b64\\jember_model_W7_E114_B64_DO0.02.h5\n",
      "   ✓ Scaler disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\jember_w7_e114_b64\\jember_scaler_W7_E114_B64_DO0.02.pkl\n",
      "   ✓ Metadata disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\jember_w7_e114_b64\\jember_metadata.json\n",
      "   ✓ Grafik Loss REVISI disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\jember_w7_e114_b64\\jember_Loss_Graph_Revisi.png\n",
      "   ✓ Plot disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\jember_w7_e114_b64\\jember_Plot.png\n",
      "   ✓ Prediksi disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\jember_w7_e114_b64\\jember_Prediksi_W7_E114_B64.xlsx\n",
      "   ✓ Ringkasan disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\jember_w7_e114_b64\\jember_Summary_W7_E114_B64.xlsx\n",
      "\n",
      "Selesai untuk file ini.\n"
     ]
    }
   ],
   "source": [
    "#VISUALISASI LOSS DATA (REVISI FINAL)\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(42)\n",
    "    python_random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "reset_seeds()\n",
    "\n",
    "\n",
    "FIXED_WINDOW_SIZE = 7\n",
    "FIXED_EPOCH = 114\n",
    "FIXED_BATCH_SIZE = 64\n",
    "VAL_RATIO = 0.1\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "path_file_input = r\"E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_jember.xlsx\"\n",
    "path_output_folder = r\"E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\"\n",
    "os.makedirs(path_output_folder, exist_ok=True)\n",
    "\n",
    "def create_sliding_window(dataset, window_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        X.append(dataset[i:(i + window_size), 0])\n",
    "        Y.append(dataset[i + window_size, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def build_model_lstm(input_shape):\n",
    "    tf.random.set_seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "if not os.path.exists(path_file_input):\n",
    "    raise FileNotFoundError(f\"File tidak ditemukan: {path_file_input}\")\n",
    "\n",
    "df = pd.read_excel(path_file_input)\n",
    "nama_kolom_tanggal = 'Tanggal' \n",
    "\n",
    "if 'Harga (Rp)' not in df.columns:\n",
    "    raise KeyError(f\"Kolom 'Harga (Rp)' tidak ditemukan di file {path_file_input}\")\n",
    "\n",
    "df[nama_kolom_tanggal] = pd.to_datetime(df[nama_kolom_tanggal])\n",
    "raw_data = df['Harga (Rp)'].values.reshape(-1, 1)\n",
    "print(f\"✓ Data: {path_file_input}\")\n",
    "print(f\"✓ Jumlah baris asli (raw): {len(raw_data)}\")\n",
    "\n",
    "region_name = os.path.basename(path_file_input).replace(\"Data_Clean_\", \"\").replace(\".xlsx\", \"\")\n",
    "\n",
    "X_full_raw, Y_full_raw = create_sliding_window(raw_data, FIXED_WINDOW_SIZE)\n",
    "print(f\"✓ Sampel window: {len(X_full_raw)} (window={FIXED_WINDOW_SIZE})\")\n",
    "train_size = int(len(X_full_raw) * 0.8)\n",
    "y_test_orig = Y_full_raw[train_size:]  \n",
    "\n",
    "\n",
    "raw_train_segment = raw_data[:train_size + FIXED_WINDOW_SIZE]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(raw_train_segment)\n",
    "scaled_data = scaler.transform(raw_data)\n",
    "print(\"✓ Data ternormalisasi (0-1).\")\n",
    "\n",
    "\n",
    "X_full_scaled, Y_full_scaled = create_sliding_window(scaled_data, FIXED_WINDOW_SIZE)\n",
    "X_full_scaled = X_full_scaled.reshape(X_full_scaled.shape[0], X_full_scaled.shape[1], 1)\n",
    "\n",
    "X_train_all, X_test = X_full_scaled[:train_size], X_full_scaled[train_size:]\n",
    "y_train_all = Y_full_scaled[:train_size]\n",
    "\n",
    "# Validasi tail dari TRAIN\n",
    "val_cut = int(len(X_train_all) * (1 - VAL_RATIO))\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "print(f\"[Info] Total: {len(X_full_scaled)} | Train: {len(X_train_all)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "\n",
    "reset_seeds()\n",
    "model = build_model_lstm((X_train.shape[1], 1))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=FIXED_EPOCH,\n",
    "    batch_size=FIXED_BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss', None)\n",
    "last_loss = float(train_loss[-1])\n",
    "last_val_loss = float(val_loss[-1]) if val_loss is not None else None\n",
    "best_epoch = int(np.argmin(val_loss) + 1) if val_loss is not None else int(np.argmin(train_loss) + 1)\n",
    "best_val = float(np.min(val_loss)) if val_loss is not None else float(np.min(train_loss))\n",
    "\n",
    "print(f\"   -> Selesai epoch : {FIXED_EPOCH}\")\n",
    "print(f\"   -> Best epoch    : {best_epoch} (val_loss={best_val:.6f})\")\n",
    "print(f\"   -> Last loss     : {last_loss:.6f}\" + (f\" | Last val_loss : {last_val_loss:.6f}\" if last_val_loss is not None else \"\"))\n",
    "\n",
    "\n",
    "predictions_scaled = model.predict(X_test, verbose=0)\n",
    "predictions_real = scaler.inverse_transform(predictions_scaled)\n",
    "mape = mean_absolute_percentage_error(y_test_orig.ravel(), predictions_real.ravel()) * 100\n",
    "print(f\"   -> MAPE          : {mape:.4f}%\")\n",
    "\n",
    "\n",
    "region_out_dir = os.path.join(path_output_folder, f\"{region_name}_w{FIXED_WINDOW_SIZE}_e{FIXED_EPOCH}_b{FIXED_BATCH_SIZE}\")\n",
    "os.makedirs(region_out_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(region_out_dir, f\"{region_name}_model_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.h5\")\n",
    "scaler_path = os.path.join(region_out_dir, f\"{region_name}_scaler_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.pkl\")\n",
    "meta_path = os.path.join(region_out_dir, f\"{region_name}_metadata.json\")\n",
    "\n",
    "model.save(model_path)\n",
    "print(f\"   ✓ Model disimpan: {model_path}\")\n",
    "\n",
    "# Simpan scaler\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"   ✓ Scaler disimpan: {scaler_path}\")\n",
    "\n",
    "metadata = {\n",
    "    \"region\": region_name,\n",
    "    \"window_size\": int(FIXED_WINDOW_SIZE),\n",
    "    \"epoch\": int(FIXED_EPOCH),\n",
    "    \"batch_size\": int(FIXED_BATCH_SIZE),\n",
    "    \"dropout_rate\": float(DROPOUT_RATE),\n",
    "    \"learning_rate\": float(LEARNING_RATE),\n",
    "    \"val_ratio\": float(VAL_RATIO),\n",
    "    \"train_samples\": int(len(X_train_all)),\n",
    "    \"val_samples\": int(len(X_val)),\n",
    "    \"test_samples\": int(len(X_test)),\n",
    "    \"best_epoch_val\": int(best_epoch),\n",
    "    \"best_val_loss\": float(best_val),\n",
    "    \"last_loss\": float(last_loss),\n",
    "    \"last_val_loss\": float(last_val_loss) if last_val_loss is not None else None,\n",
    "    \"mape_test_percent\": float(mape)\n",
    "}\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"   ✓ Metadata disimpan: {meta_path}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "\n",
    "# Plot Garis\n",
    "plt.plot(history.history['loss'], label='Training Loss (Data Latih)', linewidth=2)\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss (Data Validasi)', linewidth=2)\n",
    "\n",
    "plt.xlim(0, FIXED_EPOCH)\n",
    "ticks = list(range(0, FIXED_EPOCH, 20)) + [FIXED_EPOCH]\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.title(f'Grafik Penurunan Error (Loss) | {region_name}\\n(Semakin turun mendekati 0 = Semakin Akurat)', fontsize=14)\n",
    "plt.ylabel('Tingkat Error (MSE - Skala Normalisasi 0-1)', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Simpan\n",
    "loss_plot_path = os.path.join(region_out_dir, f\"{region_name}_Loss_Graph_Revisi.png\")\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Grafik Loss REVISI disimpan: {loss_plot_path}\")\n",
    "\n",
    "\n",
    "all_dates = df[nama_kolom_tanggal].values[FIXED_WINDOW_SIZE:]\n",
    "\n",
    "# Ambil tanggal khusus untuk bagian TEST saja\n",
    "test_dates = all_dates[train_size:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "plt.plot(test_dates, y_test_orig, label='Actual (Real Data)')\n",
    "plt.plot(test_dates, predictions_real, label='Predicted (Denormalized)')\n",
    "\n",
    "plt.title(f'{region_name} | W{FIXED_WINDOW_SIZE} E{FIXED_EPOCH} B{FIXED_BATCH_SIZE} DO{DROPOUT_RATE}')\n",
    "plt.xlabel('Tanggal dan Tahun')\n",
    "plt.ylabel('Harga (Rp)')\n",
    "\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) \n",
    "plt.gcf().autofmt_xdate() \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plot_path = os.path.join(region_out_dir, f\"{region_name}_Plot.png\")\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Plot disimpan: {plot_path}\")\n",
    "\n",
    "y_test_safe = np.where(y_test_orig.flatten() == 0, np.finfo(float).eps, y_test_orig.flatten())\n",
    "err_pct = np.abs((y_test_orig.flatten() - predictions_real.flatten()) / y_test_safe) * 100\n",
    "out_df = pd.DataFrame({\n",
    "    'Tanggal': test_dates,  \n",
    "    'Actual (Real)': y_test_orig.flatten(),\n",
    "    'Predicted (Real)': predictions_real.flatten(),\n",
    "    'Selisih': (y_test_orig.flatten() - predictions_real.flatten()),\n",
    "    'Error (%)': err_pct\n",
    "})\n",
    "excel_path = os.path.join(region_out_dir, f\"{region_name}_Prediksi_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "out_df.to_excel(excel_path, index=False)\n",
    "print(f\"   ✓ Prediksi disimpan: {excel_path}\")\n",
    "\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'Region': region_name,\n",
    "    'Window Size': int(FIXED_WINDOW_SIZE),\n",
    "    'Epoch': int(FIXED_EPOCH),\n",
    "    'Batch Size': int(FIXED_BATCH_SIZE),\n",
    "    'Dropout': float(DROPOUT_RATE),\n",
    "    'Best Epoch (val)': int(best_epoch),\n",
    "    'Best Val Loss': float(best_val),\n",
    "    'Last Loss': float(last_loss),\n",
    "    'Last Val Loss': float(last_val_loss) if last_val_loss is not None else None,\n",
    "    'MAPE (%)': float(mape)\n",
    "}])\n",
    "summary_path = os.path.join(region_out_dir, f\"{region_name}_Summary_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "summary.to_excel(summary_path, index=False)\n",
    "print(f\"   ✓ Ringkasan disimpan: {summary_path}\")\n",
    "\n",
    "print(\"\\nSelesai untuk file ini.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "973f2a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data: E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_kediri.xlsx\n",
      "✓ Jumlah baris asli (raw): 1043\n",
      "✓ Sampel window: 1036 (window=7)\n",
      "✓ Data ternormalisasi (0-1).\n",
      "[Info] Total: 1036 | Train: 828 | Val: 83 | Test: 208\n",
      "   -> Selesai epoch : 114\n",
      "   -> Best epoch    : 111 (val_loss=0.000212)\n",
      "   -> Last loss     : 0.000511 | Last val_loss : 0.000216\n",
      "   -> MAPE          : 2.2605%\n",
      "   ✓ Model disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\kediri_w7_e114_b64\\kediri_model_W7_E114_B64_DO0.02.h5\n",
      "   ✓ Scaler disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\kediri_w7_e114_b64\\kediri_scaler_W7_E114_B64_DO0.02.pkl\n",
      "   ✓ Metadata disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\kediri_w7_e114_b64\\kediri_metadata.json\n",
      "   ✓ Grafik Loss REVISI disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\kediri_w7_e114_b64\\kediri_Loss_Graph_Revisi.png\n",
      "   ✓ Plot disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\kediri_w7_e114_b64\\kediri_Plot.png\n",
      "   ✓ Prediksi disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\kediri_w7_e114_b64\\kediri_Prediksi_W7_E114_B64.xlsx\n",
      "   ✓ Ringkasan disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\kediri_w7_e114_b64\\kediri_Summary_W7_E114_B64.xlsx\n",
      "\n",
      "Selesai untuk file ini.\n"
     ]
    }
   ],
   "source": [
    "#VISUALISASI LOSS DATA (REVISI FINAL)\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(42)\n",
    "    python_random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "reset_seeds()\n",
    "\n",
    "\n",
    "FIXED_WINDOW_SIZE = 7\n",
    "FIXED_EPOCH = 114\n",
    "FIXED_BATCH_SIZE = 64\n",
    "VAL_RATIO = 0.1\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "path_file_input = r\"E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_kediri.xlsx\"\n",
    "path_output_folder = r\"E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\"\n",
    "os.makedirs(path_output_folder, exist_ok=True)\n",
    "\n",
    "def create_sliding_window(dataset, window_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        X.append(dataset[i:(i + window_size), 0])\n",
    "        Y.append(dataset[i + window_size, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def build_model_lstm(input_shape):\n",
    "    tf.random.set_seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "if not os.path.exists(path_file_input):\n",
    "    raise FileNotFoundError(f\"File tidak ditemukan: {path_file_input}\")\n",
    "\n",
    "df = pd.read_excel(path_file_input)\n",
    "nama_kolom_tanggal = 'Tanggal' \n",
    "\n",
    "if 'Harga (Rp)' not in df.columns:\n",
    "    raise KeyError(f\"Kolom 'Harga (Rp)' tidak ditemukan di file {path_file_input}\")\n",
    "\n",
    "df[nama_kolom_tanggal] = pd.to_datetime(df[nama_kolom_tanggal])\n",
    "raw_data = df['Harga (Rp)'].values.reshape(-1, 1)\n",
    "print(f\"✓ Data: {path_file_input}\")\n",
    "print(f\"✓ Jumlah baris asli (raw): {len(raw_data)}\")\n",
    "\n",
    "region_name = os.path.basename(path_file_input).replace(\"Data_Clean_\", \"\").replace(\".xlsx\", \"\")\n",
    "\n",
    "X_full_raw, Y_full_raw = create_sliding_window(raw_data, FIXED_WINDOW_SIZE)\n",
    "print(f\"✓ Sampel window: {len(X_full_raw)} (window={FIXED_WINDOW_SIZE})\")\n",
    "train_size = int(len(X_full_raw) * 0.8)\n",
    "y_test_orig = Y_full_raw[train_size:]  \n",
    "\n",
    "\n",
    "raw_train_segment = raw_data[:train_size + FIXED_WINDOW_SIZE]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(raw_train_segment)\n",
    "scaled_data = scaler.transform(raw_data)\n",
    "print(\"✓ Data ternormalisasi (0-1).\")\n",
    "\n",
    "\n",
    "X_full_scaled, Y_full_scaled = create_sliding_window(scaled_data, FIXED_WINDOW_SIZE)\n",
    "X_full_scaled = X_full_scaled.reshape(X_full_scaled.shape[0], X_full_scaled.shape[1], 1)\n",
    "\n",
    "X_train_all, X_test = X_full_scaled[:train_size], X_full_scaled[train_size:]\n",
    "y_train_all = Y_full_scaled[:train_size]\n",
    "\n",
    "# Validasi tail dari TRAIN\n",
    "val_cut = int(len(X_train_all) * (1 - VAL_RATIO))\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "print(f\"[Info] Total: {len(X_full_scaled)} | Train: {len(X_train_all)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "\n",
    "reset_seeds()\n",
    "model = build_model_lstm((X_train.shape[1], 1))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=FIXED_EPOCH,\n",
    "    batch_size=FIXED_BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss', None)\n",
    "last_loss = float(train_loss[-1])\n",
    "last_val_loss = float(val_loss[-1]) if val_loss is not None else None\n",
    "best_epoch = int(np.argmin(val_loss) + 1) if val_loss is not None else int(np.argmin(train_loss) + 1)\n",
    "best_val = float(np.min(val_loss)) if val_loss is not None else float(np.min(train_loss))\n",
    "\n",
    "print(f\"   -> Selesai epoch : {FIXED_EPOCH}\")\n",
    "print(f\"   -> Best epoch    : {best_epoch} (val_loss={best_val:.6f})\")\n",
    "print(f\"   -> Last loss     : {last_loss:.6f}\" + (f\" | Last val_loss : {last_val_loss:.6f}\" if last_val_loss is not None else \"\"))\n",
    "\n",
    "\n",
    "predictions_scaled = model.predict(X_test, verbose=0)\n",
    "predictions_real = scaler.inverse_transform(predictions_scaled)\n",
    "mape = mean_absolute_percentage_error(y_test_orig.ravel(), predictions_real.ravel()) * 100\n",
    "print(f\"   -> MAPE          : {mape:.4f}%\")\n",
    "\n",
    "\n",
    "region_out_dir = os.path.join(path_output_folder, f\"{region_name}_w{FIXED_WINDOW_SIZE}_e{FIXED_EPOCH}_b{FIXED_BATCH_SIZE}\")\n",
    "os.makedirs(region_out_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(region_out_dir, f\"{region_name}_model_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.h5\")\n",
    "scaler_path = os.path.join(region_out_dir, f\"{region_name}_scaler_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.pkl\")\n",
    "meta_path = os.path.join(region_out_dir, f\"{region_name}_metadata.json\")\n",
    "\n",
    "model.save(model_path)\n",
    "print(f\"   ✓ Model disimpan: {model_path}\")\n",
    "\n",
    "# Simpan scaler\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"   ✓ Scaler disimpan: {scaler_path}\")\n",
    "\n",
    "metadata = {\n",
    "    \"region\": region_name,\n",
    "    \"window_size\": int(FIXED_WINDOW_SIZE),\n",
    "    \"epoch\": int(FIXED_EPOCH),\n",
    "    \"batch_size\": int(FIXED_BATCH_SIZE),\n",
    "    \"dropout_rate\": float(DROPOUT_RATE),\n",
    "    \"learning_rate\": float(LEARNING_RATE),\n",
    "    \"val_ratio\": float(VAL_RATIO),\n",
    "    \"train_samples\": int(len(X_train_all)),\n",
    "    \"val_samples\": int(len(X_val)),\n",
    "    \"test_samples\": int(len(X_test)),\n",
    "    \"best_epoch_val\": int(best_epoch),\n",
    "    \"best_val_loss\": float(best_val),\n",
    "    \"last_loss\": float(last_loss),\n",
    "    \"last_val_loss\": float(last_val_loss) if last_val_loss is not None else None,\n",
    "    \"mape_test_percent\": float(mape)\n",
    "}\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"   ✓ Metadata disimpan: {meta_path}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "\n",
    "# Plot Garis\n",
    "plt.plot(history.history['loss'], label='Training Loss (Data Latih)', linewidth=2)\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss (Data Validasi)', linewidth=2)\n",
    "\n",
    "plt.xlim(0, FIXED_EPOCH)\n",
    "ticks = list(range(0, FIXED_EPOCH, 20)) + [FIXED_EPOCH]\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.title(f'Grafik Penurunan Error (Loss) | {region_name}\\n(Semakin turun mendekati 0 = Semakin Akurat)', fontsize=14)\n",
    "plt.ylabel('Tingkat Error (MSE - Skala Normalisasi 0-1)', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Simpan\n",
    "loss_plot_path = os.path.join(region_out_dir, f\"{region_name}_Loss_Graph_Revisi.png\")\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Grafik Loss REVISI disimpan: {loss_plot_path}\")\n",
    "\n",
    "\n",
    "all_dates = df[nama_kolom_tanggal].values[FIXED_WINDOW_SIZE:]\n",
    "\n",
    "# Ambil tanggal khusus untuk bagian TEST saja\n",
    "test_dates = all_dates[train_size:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "plt.plot(test_dates, y_test_orig, label='Actual (Real Data)')\n",
    "plt.plot(test_dates, predictions_real, label='Predicted (Denormalized)')\n",
    "\n",
    "plt.title(f'{region_name} | W{FIXED_WINDOW_SIZE} E{FIXED_EPOCH} B{FIXED_BATCH_SIZE} DO{DROPOUT_RATE}')\n",
    "plt.xlabel('Tanggal dan Tahun')\n",
    "plt.ylabel('Harga (Rp)')\n",
    "\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) \n",
    "plt.gcf().autofmt_xdate() \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plot_path = os.path.join(region_out_dir, f\"{region_name}_Plot.png\")\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Plot disimpan: {plot_path}\")\n",
    "\n",
    "y_test_safe = np.where(y_test_orig.flatten() == 0, np.finfo(float).eps, y_test_orig.flatten())\n",
    "err_pct = np.abs((y_test_orig.flatten() - predictions_real.flatten()) / y_test_safe) * 100\n",
    "out_df = pd.DataFrame({\n",
    "    'Tanggal': test_dates,  \n",
    "    'Actual (Real)': y_test_orig.flatten(),\n",
    "    'Predicted (Real)': predictions_real.flatten(),\n",
    "    'Selisih': (y_test_orig.flatten() - predictions_real.flatten()),\n",
    "    'Error (%)': err_pct\n",
    "})\n",
    "excel_path = os.path.join(region_out_dir, f\"{region_name}_Prediksi_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "out_df.to_excel(excel_path, index=False)\n",
    "print(f\"   ✓ Prediksi disimpan: {excel_path}\")\n",
    "\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'Region': region_name,\n",
    "    'Window Size': int(FIXED_WINDOW_SIZE),\n",
    "    'Epoch': int(FIXED_EPOCH),\n",
    "    'Batch Size': int(FIXED_BATCH_SIZE),\n",
    "    'Dropout': float(DROPOUT_RATE),\n",
    "    'Best Epoch (val)': int(best_epoch),\n",
    "    'Best Val Loss': float(best_val),\n",
    "    'Last Loss': float(last_loss),\n",
    "    'Last Val Loss': float(last_val_loss) if last_val_loss is not None else None,\n",
    "    'MAPE (%)': float(mape)\n",
    "}])\n",
    "summary_path = os.path.join(region_out_dir, f\"{region_name}_Summary_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "summary.to_excel(summary_path, index=False)\n",
    "print(f\"   ✓ Ringkasan disimpan: {summary_path}\")\n",
    "\n",
    "print(\"\\nSelesai untuk file ini.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7d7f922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data: E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Madiun.xlsx\n",
      "✓ Jumlah baris asli (raw): 1043\n",
      "✓ Sampel window: 1036 (window=7)\n",
      "✓ Data ternormalisasi (0-1).\n",
      "[Info] Total: 1036 | Train: 828 | Val: 83 | Test: 208\n",
      "   -> Selesai epoch : 114\n",
      "   -> Best epoch    : 113 (val_loss=0.000370)\n",
      "   -> Last loss     : 0.000936 | Last val_loss : 0.000710\n",
      "   -> MAPE          : 4.5632%\n",
      "   ✓ Model disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Madiun_w7_e114_b64\\Madiun_model_W7_E114_B64_DO0.02.h5\n",
      "   ✓ Scaler disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Madiun_w7_e114_b64\\Madiun_scaler_W7_E114_B64_DO0.02.pkl\n",
      "   ✓ Metadata disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Madiun_w7_e114_b64\\Madiun_metadata.json\n",
      "   ✓ Grafik Loss REVISI disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Madiun_w7_e114_b64\\Madiun_Loss_Graph_Revisi.png\n",
      "   ✓ Plot disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Madiun_w7_e114_b64\\Madiun_Plot.png\n",
      "   ✓ Prediksi disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Madiun_w7_e114_b64\\Madiun_Prediksi_W7_E114_B64.xlsx\n",
      "   ✓ Ringkasan disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Madiun_w7_e114_b64\\Madiun_Summary_W7_E114_B64.xlsx\n",
      "\n",
      "Selesai untuk file ini.\n"
     ]
    }
   ],
   "source": [
    "#VISUALISASI LOSS DATA (REVISI FINAL)\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(42)\n",
    "    python_random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "reset_seeds()\n",
    "\n",
    "\n",
    "FIXED_WINDOW_SIZE = 7\n",
    "FIXED_EPOCH = 114\n",
    "FIXED_BATCH_SIZE = 64\n",
    "VAL_RATIO = 0.1\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "path_file_input = r\"E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Madiun.xlsx\"\n",
    "path_output_folder = r\"E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\"\n",
    "os.makedirs(path_output_folder, exist_ok=True)\n",
    "\n",
    "def create_sliding_window(dataset, window_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        X.append(dataset[i:(i + window_size), 0])\n",
    "        Y.append(dataset[i + window_size, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def build_model_lstm(input_shape):\n",
    "    tf.random.set_seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "if not os.path.exists(path_file_input):\n",
    "    raise FileNotFoundError(f\"File tidak ditemukan: {path_file_input}\")\n",
    "\n",
    "df = pd.read_excel(path_file_input)\n",
    "nama_kolom_tanggal = 'Tanggal' \n",
    "\n",
    "if 'Harga (Rp)' not in df.columns:\n",
    "    raise KeyError(f\"Kolom 'Harga (Rp)' tidak ditemukan di file {path_file_input}\")\n",
    "\n",
    "df[nama_kolom_tanggal] = pd.to_datetime(df[nama_kolom_tanggal])\n",
    "raw_data = df['Harga (Rp)'].values.reshape(-1, 1)\n",
    "print(f\"✓ Data: {path_file_input}\")\n",
    "print(f\"✓ Jumlah baris asli (raw): {len(raw_data)}\")\n",
    "\n",
    "region_name = os.path.basename(path_file_input).replace(\"Data_Clean_\", \"\").replace(\".xlsx\", \"\")\n",
    "\n",
    "X_full_raw, Y_full_raw = create_sliding_window(raw_data, FIXED_WINDOW_SIZE)\n",
    "print(f\"✓ Sampel window: {len(X_full_raw)} (window={FIXED_WINDOW_SIZE})\")\n",
    "train_size = int(len(X_full_raw) * 0.8)\n",
    "y_test_orig = Y_full_raw[train_size:]  \n",
    "\n",
    "\n",
    "raw_train_segment = raw_data[:train_size + FIXED_WINDOW_SIZE]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(raw_train_segment)\n",
    "scaled_data = scaler.transform(raw_data)\n",
    "print(\"✓ Data ternormalisasi (0-1).\")\n",
    "\n",
    "\n",
    "X_full_scaled, Y_full_scaled = create_sliding_window(scaled_data, FIXED_WINDOW_SIZE)\n",
    "X_full_scaled = X_full_scaled.reshape(X_full_scaled.shape[0], X_full_scaled.shape[1], 1)\n",
    "\n",
    "X_train_all, X_test = X_full_scaled[:train_size], X_full_scaled[train_size:]\n",
    "y_train_all = Y_full_scaled[:train_size]\n",
    "\n",
    "# Validasi tail dari TRAIN\n",
    "val_cut = int(len(X_train_all) * (1 - VAL_RATIO))\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "print(f\"[Info] Total: {len(X_full_scaled)} | Train: {len(X_train_all)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "\n",
    "reset_seeds()\n",
    "model = build_model_lstm((X_train.shape[1], 1))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=FIXED_EPOCH,\n",
    "    batch_size=FIXED_BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss', None)\n",
    "last_loss = float(train_loss[-1])\n",
    "last_val_loss = float(val_loss[-1]) if val_loss is not None else None\n",
    "best_epoch = int(np.argmin(val_loss) + 1) if val_loss is not None else int(np.argmin(train_loss) + 1)\n",
    "best_val = float(np.min(val_loss)) if val_loss is not None else float(np.min(train_loss))\n",
    "\n",
    "print(f\"   -> Selesai epoch : {FIXED_EPOCH}\")\n",
    "print(f\"   -> Best epoch    : {best_epoch} (val_loss={best_val:.6f})\")\n",
    "print(f\"   -> Last loss     : {last_loss:.6f}\" + (f\" | Last val_loss : {last_val_loss:.6f}\" if last_val_loss is not None else \"\"))\n",
    "\n",
    "\n",
    "predictions_scaled = model.predict(X_test, verbose=0)\n",
    "predictions_real = scaler.inverse_transform(predictions_scaled)\n",
    "mape = mean_absolute_percentage_error(y_test_orig.ravel(), predictions_real.ravel()) * 100\n",
    "print(f\"   -> MAPE          : {mape:.4f}%\")\n",
    "\n",
    "\n",
    "region_out_dir = os.path.join(path_output_folder, f\"{region_name}_w{FIXED_WINDOW_SIZE}_e{FIXED_EPOCH}_b{FIXED_BATCH_SIZE}\")\n",
    "os.makedirs(region_out_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(region_out_dir, f\"{region_name}_model_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.h5\")\n",
    "scaler_path = os.path.join(region_out_dir, f\"{region_name}_scaler_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.pkl\")\n",
    "meta_path = os.path.join(region_out_dir, f\"{region_name}_metadata.json\")\n",
    "\n",
    "model.save(model_path)\n",
    "print(f\"   ✓ Model disimpan: {model_path}\")\n",
    "\n",
    "# Simpan scaler\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"   ✓ Scaler disimpan: {scaler_path}\")\n",
    "\n",
    "metadata = {\n",
    "    \"region\": region_name,\n",
    "    \"window_size\": int(FIXED_WINDOW_SIZE),\n",
    "    \"epoch\": int(FIXED_EPOCH),\n",
    "    \"batch_size\": int(FIXED_BATCH_SIZE),\n",
    "    \"dropout_rate\": float(DROPOUT_RATE),\n",
    "    \"learning_rate\": float(LEARNING_RATE),\n",
    "    \"val_ratio\": float(VAL_RATIO),\n",
    "    \"train_samples\": int(len(X_train_all)),\n",
    "    \"val_samples\": int(len(X_val)),\n",
    "    \"test_samples\": int(len(X_test)),\n",
    "    \"best_epoch_val\": int(best_epoch),\n",
    "    \"best_val_loss\": float(best_val),\n",
    "    \"last_loss\": float(last_loss),\n",
    "    \"last_val_loss\": float(last_val_loss) if last_val_loss is not None else None,\n",
    "    \"mape_test_percent\": float(mape)\n",
    "}\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"   ✓ Metadata disimpan: {meta_path}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "\n",
    "# Plot Garis\n",
    "plt.plot(history.history['loss'], label='Training Loss (Data Latih)', linewidth=2)\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss (Data Validasi)', linewidth=2)\n",
    "\n",
    "plt.xlim(0, FIXED_EPOCH)\n",
    "ticks = list(range(0, FIXED_EPOCH, 20)) + [FIXED_EPOCH]\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.title(f'Grafik Penurunan Error (Loss) | {region_name}\\n(Semakin turun mendekati 0 = Semakin Akurat)', fontsize=14)\n",
    "plt.ylabel('Tingkat Error (MSE - Skala Normalisasi 0-1)', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Simpan\n",
    "loss_plot_path = os.path.join(region_out_dir, f\"{region_name}_Loss_Graph_Revisi.png\")\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Grafik Loss REVISI disimpan: {loss_plot_path}\")\n",
    "\n",
    "\n",
    "all_dates = df[nama_kolom_tanggal].values[FIXED_WINDOW_SIZE:]\n",
    "\n",
    "# Ambil tanggal khusus untuk bagian TEST saja\n",
    "test_dates = all_dates[train_size:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "plt.plot(test_dates, y_test_orig, label='Actual (Real Data)')\n",
    "plt.plot(test_dates, predictions_real, label='Predicted (Denormalized)')\n",
    "\n",
    "plt.title(f'{region_name} | W{FIXED_WINDOW_SIZE} E{FIXED_EPOCH} B{FIXED_BATCH_SIZE} DO{DROPOUT_RATE}')\n",
    "plt.xlabel('Tanggal dan Tahun')\n",
    "plt.ylabel('Harga (Rp)')\n",
    "\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) \n",
    "plt.gcf().autofmt_xdate() \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plot_path = os.path.join(region_out_dir, f\"{region_name}_Plot.png\")\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Plot disimpan: {plot_path}\")\n",
    "\n",
    "y_test_safe = np.where(y_test_orig.flatten() == 0, np.finfo(float).eps, y_test_orig.flatten())\n",
    "err_pct = np.abs((y_test_orig.flatten() - predictions_real.flatten()) / y_test_safe) * 100\n",
    "out_df = pd.DataFrame({\n",
    "    'Tanggal': test_dates,  \n",
    "    'Actual (Real)': y_test_orig.flatten(),\n",
    "    'Predicted (Real)': predictions_real.flatten(),\n",
    "    'Selisih': (y_test_orig.flatten() - predictions_real.flatten()),\n",
    "    'Error (%)': err_pct\n",
    "})\n",
    "excel_path = os.path.join(region_out_dir, f\"{region_name}_Prediksi_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "out_df.to_excel(excel_path, index=False)\n",
    "print(f\"   ✓ Prediksi disimpan: {excel_path}\")\n",
    "\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'Region': region_name,\n",
    "    'Window Size': int(FIXED_WINDOW_SIZE),\n",
    "    'Epoch': int(FIXED_EPOCH),\n",
    "    'Batch Size': int(FIXED_BATCH_SIZE),\n",
    "    'Dropout': float(DROPOUT_RATE),\n",
    "    'Best Epoch (val)': int(best_epoch),\n",
    "    'Best Val Loss': float(best_val),\n",
    "    'Last Loss': float(last_loss),\n",
    "    'Last Val Loss': float(last_val_loss) if last_val_loss is not None else None,\n",
    "    'MAPE (%)': float(mape)\n",
    "}])\n",
    "summary_path = os.path.join(region_out_dir, f\"{region_name}_Summary_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "summary.to_excel(summary_path, index=False)\n",
    "print(f\"   ✓ Ringkasan disimpan: {summary_path}\")\n",
    "\n",
    "print(\"\\nSelesai untuk file ini.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3de028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data: E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Malang.xlsx\n",
      "✓ Jumlah baris asli (raw): 1043\n",
      "✓ Sampel window: 1036 (window=7)\n",
      "✓ Data ternormalisasi (0-1).\n",
      "[Info] Total: 1036 | Train: 828 | Val: 83 | Test: 208\n",
      "   -> Selesai epoch : 114\n",
      "   -> Best epoch    : 99 (val_loss=0.000196)\n",
      "   -> Last loss     : 0.000923 | Last val_loss : 0.000863\n",
      "   -> MAPE          : 4.6465%\n",
      "   ✓ Model disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Malang_w7_e114_b64\\Malang_model_W7_E114_B64_DO0.02.h5\n",
      "   ✓ Scaler disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Malang_w7_e114_b64\\Malang_scaler_W7_E114_B64_DO0.02.pkl\n",
      "   ✓ Metadata disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Malang_w7_e114_b64\\Malang_metadata.json\n",
      "   ✓ Grafik Loss REVISI disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Malang_w7_e114_b64\\Malang_Loss_Graph_Revisi.png\n",
      "   ✓ Plot disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Malang_w7_e114_b64\\Malang_Plot.png\n",
      "   ✓ Prediksi disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Malang_w7_e114_b64\\Malang_Prediksi_W7_E114_B64.xlsx\n",
      "   ✓ Ringkasan disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Malang_w7_e114_b64\\Malang_Summary_W7_E114_B64.xlsx\n",
      "\n",
      "Selesai untuk file ini.\n"
     ]
    }
   ],
   "source": [
    "#VISUALISASI LOSS DATA (REVISI FINAL)\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(42)\n",
    "    python_random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "reset_seeds()\n",
    "\n",
    "\n",
    "FIXED_WINDOW_SIZE = 7\n",
    "FIXED_EPOCH = 114\n",
    "FIXED_BATCH_SIZE = 64\n",
    "VAL_RATIO = 0.1\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "path_file_input = r\"E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Malang.xlsx\"\n",
    "path_output_folder = r\"E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\"\n",
    "os.makedirs(path_output_folder, exist_ok=True)\n",
    "\n",
    "def create_sliding_window(dataset, window_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        X.append(dataset[i:(i + window_size), 0])\n",
    "        Y.append(dataset[i + window_size, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def build_model_lstm(input_shape):\n",
    "    tf.random.set_seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "if not os.path.exists(path_file_input):\n",
    "    raise FileNotFoundError(f\"File tidak ditemukan: {path_file_input}\")\n",
    "\n",
    "df = pd.read_excel(path_file_input)\n",
    "nama_kolom_tanggal = 'Tanggal' \n",
    "\n",
    "if 'Harga (Rp)' not in df.columns:\n",
    "    raise KeyError(f\"Kolom 'Harga (Rp)' tidak ditemukan di file {path_file_input}\")\n",
    "\n",
    "df[nama_kolom_tanggal] = pd.to_datetime(df[nama_kolom_tanggal])\n",
    "raw_data = df['Harga (Rp)'].values.reshape(-1, 1)\n",
    "print(f\"✓ Data: {path_file_input}\")\n",
    "print(f\"✓ Jumlah baris asli (raw): {len(raw_data)}\")\n",
    "\n",
    "region_name = os.path.basename(path_file_input).replace(\"Data_Clean_\", \"\").replace(\".xlsx\", \"\")\n",
    "\n",
    "X_full_raw, Y_full_raw = create_sliding_window(raw_data, FIXED_WINDOW_SIZE)\n",
    "print(f\"✓ Sampel window: {len(X_full_raw)} (window={FIXED_WINDOW_SIZE})\")\n",
    "train_size = int(len(X_full_raw) * 0.8)\n",
    "y_test_orig = Y_full_raw[train_size:]  \n",
    "\n",
    "\n",
    "raw_train_segment = raw_data[:train_size + FIXED_WINDOW_SIZE]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(raw_train_segment)\n",
    "scaled_data = scaler.transform(raw_data)\n",
    "print(\"✓ Data ternormalisasi (0-1).\")\n",
    "\n",
    "\n",
    "X_full_scaled, Y_full_scaled = create_sliding_window(scaled_data, FIXED_WINDOW_SIZE)\n",
    "X_full_scaled = X_full_scaled.reshape(X_full_scaled.shape[0], X_full_scaled.shape[1], 1)\n",
    "\n",
    "X_train_all, X_test = X_full_scaled[:train_size], X_full_scaled[train_size:]\n",
    "y_train_all = Y_full_scaled[:train_size]\n",
    "\n",
    "# Validasi tail dari TRAIN\n",
    "val_cut = int(len(X_train_all) * (1 - VAL_RATIO))\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "print(f\"[Info] Total: {len(X_full_scaled)} | Train: {len(X_train_all)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "\n",
    "reset_seeds()\n",
    "model = build_model_lstm((X_train.shape[1], 1))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=FIXED_EPOCH,\n",
    "    batch_size=FIXED_BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss', None)\n",
    "last_loss = float(train_loss[-1])\n",
    "last_val_loss = float(val_loss[-1]) if val_loss is not None else None\n",
    "best_epoch = int(np.argmin(val_loss) + 1) if val_loss is not None else int(np.argmin(train_loss) + 1)\n",
    "best_val = float(np.min(val_loss)) if val_loss is not None else float(np.min(train_loss))\n",
    "\n",
    "print(f\"   -> Selesai epoch : {FIXED_EPOCH}\")\n",
    "print(f\"   -> Best epoch    : {best_epoch} (val_loss={best_val:.6f})\")\n",
    "print(f\"   -> Last loss     : {last_loss:.6f}\" + (f\" | Last val_loss : {last_val_loss:.6f}\" if last_val_loss is not None else \"\"))\n",
    "\n",
    "\n",
    "predictions_scaled = model.predict(X_test, verbose=0)\n",
    "predictions_real = scaler.inverse_transform(predictions_scaled)\n",
    "mape = mean_absolute_percentage_error(y_test_orig.ravel(), predictions_real.ravel()) * 100\n",
    "print(f\"   -> MAPE          : {mape:.4f}%\")\n",
    "\n",
    "\n",
    "region_out_dir = os.path.join(path_output_folder, f\"{region_name}_w{FIXED_WINDOW_SIZE}_e{FIXED_EPOCH}_b{FIXED_BATCH_SIZE}\")\n",
    "os.makedirs(region_out_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(region_out_dir, f\"{region_name}_model_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.h5\")\n",
    "scaler_path = os.path.join(region_out_dir, f\"{region_name}_scaler_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.pkl\")\n",
    "meta_path = os.path.join(region_out_dir, f\"{region_name}_metadata.json\")\n",
    "\n",
    "model.save(model_path)\n",
    "print(f\"   ✓ Model disimpan: {model_path}\")\n",
    "\n",
    "# Simpan scaler\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"   ✓ Scaler disimpan: {scaler_path}\")\n",
    "\n",
    "metadata = {\n",
    "    \"region\": region_name,\n",
    "    \"window_size\": int(FIXED_WINDOW_SIZE),\n",
    "    \"epoch\": int(FIXED_EPOCH),\n",
    "    \"batch_size\": int(FIXED_BATCH_SIZE),\n",
    "    \"dropout_rate\": float(DROPOUT_RATE),\n",
    "    \"learning_rate\": float(LEARNING_RATE),\n",
    "    \"val_ratio\": float(VAL_RATIO),\n",
    "    \"train_samples\": int(len(X_train_all)),\n",
    "    \"val_samples\": int(len(X_val)),\n",
    "    \"test_samples\": int(len(X_test)),\n",
    "    \"best_epoch_val\": int(best_epoch),\n",
    "    \"best_val_loss\": float(best_val),\n",
    "    \"last_loss\": float(last_loss),\n",
    "    \"last_val_loss\": float(last_val_loss) if last_val_loss is not None else None,\n",
    "    \"mape_test_percent\": float(mape)\n",
    "}\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"   ✓ Metadata disimpan: {meta_path}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "\n",
    "# Plot Garis\n",
    "plt.plot(history.history['loss'], label='Training Loss (Data Latih)', linewidth=2)\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss (Data Validasi)', linewidth=2)\n",
    "\n",
    "plt.xlim(0, FIXED_EPOCH)\n",
    "ticks = list(range(0, FIXED_EPOCH, 20)) + [FIXED_EPOCH]\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.title(f'Grafik Penurunan Error (Loss) | {region_name}\\n(Semakin turun mendekati 0 = Semakin Akurat)', fontsize=14)\n",
    "plt.ylabel('Tingkat Error (MSE - Skala Normalisasi 0-1)', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Simpan\n",
    "loss_plot_path = os.path.join(region_out_dir, f\"{region_name}_Loss_Graph_Revisi.png\")\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Grafik Loss REVISI disimpan: {loss_plot_path}\")\n",
    "\n",
    "\n",
    "all_dates = df[nama_kolom_tanggal].values[FIXED_WINDOW_SIZE:]\n",
    "\n",
    "# Ambil tanggal khusus untuk bagian TEST saja\n",
    "test_dates = all_dates[train_size:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "plt.plot(test_dates, y_test_orig, label='Actual (Real Data)')\n",
    "plt.plot(test_dates, predictions_real, label='Predicted (Denormalized)')\n",
    "\n",
    "plt.title(f'{region_name} | W{FIXED_WINDOW_SIZE} E{FIXED_EPOCH} B{FIXED_BATCH_SIZE} DO{DROPOUT_RATE}')\n",
    "plt.xlabel('Tanggal dan Tahun')\n",
    "plt.ylabel('Harga (Rp)')\n",
    "\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) \n",
    "plt.gcf().autofmt_xdate() \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plot_path = os.path.join(region_out_dir, f\"{region_name}_Plot.png\")\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Plot disimpan: {plot_path}\")\n",
    "\n",
    "y_test_safe = np.where(y_test_orig.flatten() == 0, np.finfo(float).eps, y_test_orig.flatten())\n",
    "err_pct = np.abs((y_test_orig.flatten() - predictions_real.flatten()) / y_test_safe) * 100\n",
    "out_df = pd.DataFrame({\n",
    "    'Tanggal': test_dates,  \n",
    "    'Actual (Real)': y_test_orig.flatten(),\n",
    "    'Predicted (Real)': predictions_real.flatten(),\n",
    "    'Selisih': (y_test_orig.flatten() - predictions_real.flatten()),\n",
    "    'Error (%)': err_pct\n",
    "})\n",
    "excel_path = os.path.join(region_out_dir, f\"{region_name}_Prediksi_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "out_df.to_excel(excel_path, index=False)\n",
    "print(f\"   ✓ Prediksi disimpan: {excel_path}\")\n",
    "\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'Region': region_name,\n",
    "    'Window Size': int(FIXED_WINDOW_SIZE),\n",
    "    'Epoch': int(FIXED_EPOCH),\n",
    "    'Batch Size': int(FIXED_BATCH_SIZE),\n",
    "    'Dropout': float(DROPOUT_RATE),\n",
    "    'Best Epoch (val)': int(best_epoch),\n",
    "    'Best Val Loss': float(best_val),\n",
    "    'Last Loss': float(last_loss),\n",
    "    'Last Val Loss': float(last_val_loss) if last_val_loss is not None else None,\n",
    "    'MAPE (%)': float(mape)\n",
    "}])\n",
    "summary_path = os.path.join(region_out_dir, f\"{region_name}_Summary_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "summary.to_excel(summary_path, index=False)\n",
    "print(f\"   ✓ Ringkasan disimpan: {summary_path}\")\n",
    "\n",
    "print(\"\\nSelesai untuk file ini.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b101578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data: E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Probolinggo.xlsx\n",
      "✓ Jumlah baris asli (raw): 1043\n",
      "✓ Sampel window: 1036 (window=7)\n",
      "✓ Data ternormalisasi (0-1).\n",
      "[Info] Total: 1036 | Train: 828 | Val: 83 | Test: 208\n",
      "   -> Selesai epoch : 114\n",
      "   -> Best epoch    : 114 (val_loss=0.000483)\n",
      "   -> Last loss     : 0.000655 | Last val_loss : 0.000483\n",
      "   -> MAPE          : 2.4025%\n",
      "   ✓ Model disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Probolinggo_w7_e114_b64\\Probolinggo_model_W7_E114_B64_DO0.02.h5\n",
      "   ✓ Scaler disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Probolinggo_w7_e114_b64\\Probolinggo_scaler_W7_E114_B64_DO0.02.pkl\n",
      "   ✓ Metadata disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Probolinggo_w7_e114_b64\\Probolinggo_metadata.json\n",
      "   ✓ Grafik Loss REVISI disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Probolinggo_w7_e114_b64\\Probolinggo_Loss_Graph_Revisi.png\n",
      "   ✓ Plot disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Probolinggo_w7_e114_b64\\Probolinggo_Plot.png\n",
      "   ✓ Prediksi disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Probolinggo_w7_e114_b64\\Probolinggo_Prediksi_W7_E114_B64.xlsx\n",
      "   ✓ Ringkasan disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Probolinggo_w7_e114_b64\\Probolinggo_Summary_W7_E114_B64.xlsx\n",
      "\n",
      "Selesai untuk file ini.\n"
     ]
    }
   ],
   "source": [
    "#VISUALISASI LOSS DATA (REVISI FINAL)\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(42)\n",
    "    python_random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "reset_seeds()\n",
    "\n",
    "\n",
    "FIXED_WINDOW_SIZE = 7\n",
    "FIXED_EPOCH = 114\n",
    "FIXED_BATCH_SIZE = 64\n",
    "VAL_RATIO = 0.1\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "path_file_input = r\"E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Probolinggo.xlsx\"\n",
    "path_output_folder = r\"E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\"\n",
    "os.makedirs(path_output_folder, exist_ok=True)\n",
    "\n",
    "def create_sliding_window(dataset, window_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        X.append(dataset[i:(i + window_size), 0])\n",
    "        Y.append(dataset[i + window_size, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def build_model_lstm(input_shape):\n",
    "    tf.random.set_seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "if not os.path.exists(path_file_input):\n",
    "    raise FileNotFoundError(f\"File tidak ditemukan: {path_file_input}\")\n",
    "\n",
    "df = pd.read_excel(path_file_input)\n",
    "nama_kolom_tanggal = 'Tanggal' \n",
    "\n",
    "if 'Harga (Rp)' not in df.columns:\n",
    "    raise KeyError(f\"Kolom 'Harga (Rp)' tidak ditemukan di file {path_file_input}\")\n",
    "\n",
    "df[nama_kolom_tanggal] = pd.to_datetime(df[nama_kolom_tanggal])\n",
    "raw_data = df['Harga (Rp)'].values.reshape(-1, 1)\n",
    "print(f\"✓ Data: {path_file_input}\")\n",
    "print(f\"✓ Jumlah baris asli (raw): {len(raw_data)}\")\n",
    "\n",
    "region_name = os.path.basename(path_file_input).replace(\"Data_Clean_\", \"\").replace(\".xlsx\", \"\")\n",
    "\n",
    "X_full_raw, Y_full_raw = create_sliding_window(raw_data, FIXED_WINDOW_SIZE)\n",
    "print(f\"✓ Sampel window: {len(X_full_raw)} (window={FIXED_WINDOW_SIZE})\")\n",
    "train_size = int(len(X_full_raw) * 0.8)\n",
    "y_test_orig = Y_full_raw[train_size:]  \n",
    "\n",
    "\n",
    "raw_train_segment = raw_data[:train_size + FIXED_WINDOW_SIZE]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(raw_train_segment)\n",
    "scaled_data = scaler.transform(raw_data)\n",
    "print(\"✓ Data ternormalisasi (0-1).\")\n",
    "\n",
    "\n",
    "X_full_scaled, Y_full_scaled = create_sliding_window(scaled_data, FIXED_WINDOW_SIZE)\n",
    "X_full_scaled = X_full_scaled.reshape(X_full_scaled.shape[0], X_full_scaled.shape[1], 1)\n",
    "\n",
    "X_train_all, X_test = X_full_scaled[:train_size], X_full_scaled[train_size:]\n",
    "y_train_all = Y_full_scaled[:train_size]\n",
    "\n",
    "# Validasi tail dari TRAIN\n",
    "val_cut = int(len(X_train_all) * (1 - VAL_RATIO))\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "print(f\"[Info] Total: {len(X_full_scaled)} | Train: {len(X_train_all)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "\n",
    "reset_seeds()\n",
    "model = build_model_lstm((X_train.shape[1], 1))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=FIXED_EPOCH,\n",
    "    batch_size=FIXED_BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss', None)\n",
    "last_loss = float(train_loss[-1])\n",
    "last_val_loss = float(val_loss[-1]) if val_loss is not None else None\n",
    "best_epoch = int(np.argmin(val_loss) + 1) if val_loss is not None else int(np.argmin(train_loss) + 1)\n",
    "best_val = float(np.min(val_loss)) if val_loss is not None else float(np.min(train_loss))\n",
    "\n",
    "print(f\"   -> Selesai epoch : {FIXED_EPOCH}\")\n",
    "print(f\"   -> Best epoch    : {best_epoch} (val_loss={best_val:.6f})\")\n",
    "print(f\"   -> Last loss     : {last_loss:.6f}\" + (f\" | Last val_loss : {last_val_loss:.6f}\" if last_val_loss is not None else \"\"))\n",
    "\n",
    "\n",
    "predictions_scaled = model.predict(X_test, verbose=0)\n",
    "predictions_real = scaler.inverse_transform(predictions_scaled)\n",
    "mape = mean_absolute_percentage_error(y_test_orig.ravel(), predictions_real.ravel()) * 100\n",
    "print(f\"   -> MAPE          : {mape:.4f}%\")\n",
    "\n",
    "\n",
    "region_out_dir = os.path.join(path_output_folder, f\"{region_name}_w{FIXED_WINDOW_SIZE}_e{FIXED_EPOCH}_b{FIXED_BATCH_SIZE}\")\n",
    "os.makedirs(region_out_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(region_out_dir, f\"{region_name}_model_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.h5\")\n",
    "scaler_path = os.path.join(region_out_dir, f\"{region_name}_scaler_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.pkl\")\n",
    "meta_path = os.path.join(region_out_dir, f\"{region_name}_metadata.json\")\n",
    "\n",
    "model.save(model_path)\n",
    "print(f\"   ✓ Model disimpan: {model_path}\")\n",
    "\n",
    "# Simpan scaler\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"   ✓ Scaler disimpan: {scaler_path}\")\n",
    "\n",
    "metadata = {\n",
    "    \"region\": region_name,\n",
    "    \"window_size\": int(FIXED_WINDOW_SIZE),\n",
    "    \"epoch\": int(FIXED_EPOCH),\n",
    "    \"batch_size\": int(FIXED_BATCH_SIZE),\n",
    "    \"dropout_rate\": float(DROPOUT_RATE),\n",
    "    \"learning_rate\": float(LEARNING_RATE),\n",
    "    \"val_ratio\": float(VAL_RATIO),\n",
    "    \"train_samples\": int(len(X_train_all)),\n",
    "    \"val_samples\": int(len(X_val)),\n",
    "    \"test_samples\": int(len(X_test)),\n",
    "    \"best_epoch_val\": int(best_epoch),\n",
    "    \"best_val_loss\": float(best_val),\n",
    "    \"last_loss\": float(last_loss),\n",
    "    \"last_val_loss\": float(last_val_loss) if last_val_loss is not None else None,\n",
    "    \"mape_test_percent\": float(mape)\n",
    "}\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"   ✓ Metadata disimpan: {meta_path}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "\n",
    "# Plot Garis\n",
    "plt.plot(history.history['loss'], label='Training Loss (Data Latih)', linewidth=2)\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss (Data Validasi)', linewidth=2)\n",
    "\n",
    "plt.xlim(0, FIXED_EPOCH)\n",
    "ticks = list(range(0, FIXED_EPOCH, 20)) + [FIXED_EPOCH]\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.title(f'Grafik Penurunan Error (Loss) | {region_name}\\n(Semakin turun mendekati 0 = Semakin Akurat)', fontsize=14)\n",
    "plt.ylabel('Tingkat Error (MSE - Skala Normalisasi 0-1)', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Simpan\n",
    "loss_plot_path = os.path.join(region_out_dir, f\"{region_name}_Loss_Graph_Revisi.png\")\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Grafik Loss REVISI disimpan: {loss_plot_path}\")\n",
    "\n",
    "\n",
    "all_dates = df[nama_kolom_tanggal].values[FIXED_WINDOW_SIZE:]\n",
    "\n",
    "# Ambil tanggal khusus untuk bagian TEST saja\n",
    "test_dates = all_dates[train_size:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "plt.plot(test_dates, y_test_orig, label='Actual (Real Data)')\n",
    "plt.plot(test_dates, predictions_real, label='Predicted (Denormalized)')\n",
    "\n",
    "plt.title(f'{region_name} | W{FIXED_WINDOW_SIZE} E{FIXED_EPOCH} B{FIXED_BATCH_SIZE} DO{DROPOUT_RATE}')\n",
    "plt.xlabel('Tanggal dan Tahun')\n",
    "plt.ylabel('Harga (Rp)')\n",
    "\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) \n",
    "plt.gcf().autofmt_xdate() \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plot_path = os.path.join(region_out_dir, f\"{region_name}_Plot.png\")\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Plot disimpan: {plot_path}\")\n",
    "\n",
    "y_test_safe = np.where(y_test_orig.flatten() == 0, np.finfo(float).eps, y_test_orig.flatten())\n",
    "err_pct = np.abs((y_test_orig.flatten() - predictions_real.flatten()) / y_test_safe) * 100\n",
    "out_df = pd.DataFrame({\n",
    "    'Tanggal': test_dates,  \n",
    "    'Actual (Real)': y_test_orig.flatten(),\n",
    "    'Predicted (Real)': predictions_real.flatten(),\n",
    "    'Selisih': (y_test_orig.flatten() - predictions_real.flatten()),\n",
    "    'Error (%)': err_pct\n",
    "})\n",
    "excel_path = os.path.join(region_out_dir, f\"{region_name}_Prediksi_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "out_df.to_excel(excel_path, index=False)\n",
    "print(f\"   ✓ Prediksi disimpan: {excel_path}\")\n",
    "\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'Region': region_name,\n",
    "    'Window Size': int(FIXED_WINDOW_SIZE),\n",
    "    'Epoch': int(FIXED_EPOCH),\n",
    "    'Batch Size': int(FIXED_BATCH_SIZE),\n",
    "    'Dropout': float(DROPOUT_RATE),\n",
    "    'Best Epoch (val)': int(best_epoch),\n",
    "    'Best Val Loss': float(best_val),\n",
    "    'Last Loss': float(last_loss),\n",
    "    'Last Val Loss': float(last_val_loss) if last_val_loss is not None else None,\n",
    "    'MAPE (%)': float(mape)\n",
    "}])\n",
    "summary_path = os.path.join(region_out_dir, f\"{region_name}_Summary_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "summary.to_excel(summary_path, index=False)\n",
    "print(f\"   ✓ Ringkasan disimpan: {summary_path}\")\n",
    "\n",
    "print(\"\\nSelesai untuk file ini.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea9768be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data: E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Sumenep.xlsx\n",
      "✓ Jumlah baris asli (raw): 1043\n",
      "✓ Sampel window: 1036 (window=7)\n",
      "✓ Data ternormalisasi (0-1).\n",
      "[Info] Total: 1036 | Train: 828 | Val: 83 | Test: 208\n",
      "   -> Selesai epoch : 114\n",
      "   -> Best epoch    : 111 (val_loss=0.000212)\n",
      "   -> Last loss     : 0.000545 | Last val_loss : 0.000242\n",
      "   -> MAPE          : 2.3627%\n",
      "   ✓ Model disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Sumenep_w7_e114_b64\\Sumenep_model_W7_E114_B64_DO0.02.h5\n",
      "   ✓ Scaler disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Sumenep_w7_e114_b64\\Sumenep_scaler_W7_E114_B64_DO0.02.pkl\n",
      "   ✓ Metadata disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Sumenep_w7_e114_b64\\Sumenep_metadata.json\n",
      "   ✓ Grafik Loss REVISI disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Sumenep_w7_e114_b64\\Sumenep_Loss_Graph_Revisi.png\n",
      "   ✓ Plot disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Sumenep_w7_e114_b64\\Sumenep_Plot.png\n",
      "   ✓ Prediksi disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Sumenep_w7_e114_b64\\Sumenep_Prediksi_W7_E114_B64.xlsx\n",
      "   ✓ Ringkasan disimpan: E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\\Sumenep_w7_e114_b64\\Sumenep_Summary_W7_E114_B64.xlsx\n",
      "\n",
      "Selesai untuk file ini.\n"
     ]
    }
   ],
   "source": [
    "#VISUALISASI LOSS DATA (REVISI FINAL)\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "def reset_seeds():\n",
    "    np.random.seed(42)\n",
    "    python_random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "reset_seeds()\n",
    "\n",
    "\n",
    "FIXED_WINDOW_SIZE = 7\n",
    "FIXED_EPOCH = 114\n",
    "FIXED_BATCH_SIZE = 64\n",
    "VAL_RATIO = 0.1\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "path_file_input = r\"E:\\Skripsi\\Prediksi-bawang-merah\\python\\data\\data clean\\Data_Clean_Sumenep.xlsx\"\n",
    "path_output_folder = r\"E:\\SKRIPSI 2025\\dataset\\VISUALISASI LOSS DATA\"\n",
    "os.makedirs(path_output_folder, exist_ok=True)\n",
    "\n",
    "def create_sliding_window(dataset, window_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        X.append(dataset[i:(i + window_size), 0])\n",
    "        Y.append(dataset[i + window_size, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def build_model_lstm(input_shape):\n",
    "    tf.random.set_seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "if not os.path.exists(path_file_input):\n",
    "    raise FileNotFoundError(f\"File tidak ditemukan: {path_file_input}\")\n",
    "\n",
    "df = pd.read_excel(path_file_input)\n",
    "nama_kolom_tanggal = 'Tanggal' \n",
    "\n",
    "if 'Harga (Rp)' not in df.columns:\n",
    "    raise KeyError(f\"Kolom 'Harga (Rp)' tidak ditemukan di file {path_file_input}\")\n",
    "\n",
    "df[nama_kolom_tanggal] = pd.to_datetime(df[nama_kolom_tanggal])\n",
    "raw_data = df['Harga (Rp)'].values.reshape(-1, 1)\n",
    "print(f\"✓ Data: {path_file_input}\")\n",
    "print(f\"✓ Jumlah baris asli (raw): {len(raw_data)}\")\n",
    "\n",
    "region_name = os.path.basename(path_file_input).replace(\"Data_Clean_\", \"\").replace(\".xlsx\", \"\")\n",
    "\n",
    "X_full_raw, Y_full_raw = create_sliding_window(raw_data, FIXED_WINDOW_SIZE)\n",
    "print(f\"✓ Sampel window: {len(X_full_raw)} (window={FIXED_WINDOW_SIZE})\")\n",
    "train_size = int(len(X_full_raw) * 0.8)\n",
    "y_test_orig = Y_full_raw[train_size:]  \n",
    "\n",
    "\n",
    "raw_train_segment = raw_data[:train_size + FIXED_WINDOW_SIZE]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(raw_train_segment)\n",
    "scaled_data = scaler.transform(raw_data)\n",
    "print(\"✓ Data ternormalisasi (0-1).\")\n",
    "\n",
    "\n",
    "X_full_scaled, Y_full_scaled = create_sliding_window(scaled_data, FIXED_WINDOW_SIZE)\n",
    "X_full_scaled = X_full_scaled.reshape(X_full_scaled.shape[0], X_full_scaled.shape[1], 1)\n",
    "\n",
    "X_train_all, X_test = X_full_scaled[:train_size], X_full_scaled[train_size:]\n",
    "y_train_all = Y_full_scaled[:train_size]\n",
    "\n",
    "# Validasi tail dari TRAIN\n",
    "val_cut = int(len(X_train_all) * (1 - VAL_RATIO))\n",
    "X_train, X_val = X_train_all[:val_cut], X_train_all[val_cut:]\n",
    "y_train, y_val = y_train_all[:val_cut], y_train_all[val_cut:]\n",
    "\n",
    "print(f\"[Info] Total: {len(X_full_scaled)} | Train: {len(X_train_all)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "\n",
    "reset_seeds()\n",
    "model = build_model_lstm((X_train.shape[1], 1))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=FIXED_EPOCH,\n",
    "    batch_size=FIXED_BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss', None)\n",
    "last_loss = float(train_loss[-1])\n",
    "last_val_loss = float(val_loss[-1]) if val_loss is not None else None\n",
    "best_epoch = int(np.argmin(val_loss) + 1) if val_loss is not None else int(np.argmin(train_loss) + 1)\n",
    "best_val = float(np.min(val_loss)) if val_loss is not None else float(np.min(train_loss))\n",
    "\n",
    "print(f\"   -> Selesai epoch : {FIXED_EPOCH}\")\n",
    "print(f\"   -> Best epoch    : {best_epoch} (val_loss={best_val:.6f})\")\n",
    "print(f\"   -> Last loss     : {last_loss:.6f}\" + (f\" | Last val_loss : {last_val_loss:.6f}\" if last_val_loss is not None else \"\"))\n",
    "\n",
    "\n",
    "predictions_scaled = model.predict(X_test, verbose=0)\n",
    "predictions_real = scaler.inverse_transform(predictions_scaled)\n",
    "mape = mean_absolute_percentage_error(y_test_orig.ravel(), predictions_real.ravel()) * 100\n",
    "print(f\"   -> MAPE          : {mape:.4f}%\")\n",
    "\n",
    "\n",
    "region_out_dir = os.path.join(path_output_folder, f\"{region_name}_w{FIXED_WINDOW_SIZE}_e{FIXED_EPOCH}_b{FIXED_BATCH_SIZE}\")\n",
    "os.makedirs(region_out_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(region_out_dir, f\"{region_name}_model_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.h5\")\n",
    "scaler_path = os.path.join(region_out_dir, f\"{region_name}_scaler_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}_DO{DROPOUT_RATE}.pkl\")\n",
    "meta_path = os.path.join(region_out_dir, f\"{region_name}_metadata.json\")\n",
    "\n",
    "model.save(model_path)\n",
    "print(f\"   ✓ Model disimpan: {model_path}\")\n",
    "\n",
    "# Simpan scaler\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"   ✓ Scaler disimpan: {scaler_path}\")\n",
    "\n",
    "metadata = {\n",
    "    \"region\": region_name,\n",
    "    \"window_size\": int(FIXED_WINDOW_SIZE),\n",
    "    \"epoch\": int(FIXED_EPOCH),\n",
    "    \"batch_size\": int(FIXED_BATCH_SIZE),\n",
    "    \"dropout_rate\": float(DROPOUT_RATE),\n",
    "    \"learning_rate\": float(LEARNING_RATE),\n",
    "    \"val_ratio\": float(VAL_RATIO),\n",
    "    \"train_samples\": int(len(X_train_all)),\n",
    "    \"val_samples\": int(len(X_val)),\n",
    "    \"test_samples\": int(len(X_test)),\n",
    "    \"best_epoch_val\": int(best_epoch),\n",
    "    \"best_val_loss\": float(best_val),\n",
    "    \"last_loss\": float(last_loss),\n",
    "    \"last_val_loss\": float(last_val_loss) if last_val_loss is not None else None,\n",
    "    \"mape_test_percent\": float(mape)\n",
    "}\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"   ✓ Metadata disimpan: {meta_path}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "\n",
    "# Plot Garis\n",
    "plt.plot(history.history['loss'], label='Training Loss (Data Latih)', linewidth=2)\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss (Data Validasi)', linewidth=2)\n",
    "\n",
    "plt.xlim(0, FIXED_EPOCH)\n",
    "ticks = list(range(0, FIXED_EPOCH, 20)) + [FIXED_EPOCH]\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.title(f'Grafik Penurunan Error (Loss) | {region_name}\\n(Semakin turun mendekati 0 = Semakin Akurat)', fontsize=14)\n",
    "plt.ylabel('Tingkat Error (MSE - Skala Normalisasi 0-1)', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Simpan\n",
    "loss_plot_path = os.path.join(region_out_dir, f\"{region_name}_Loss_Graph_Revisi.png\")\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Grafik Loss REVISI disimpan: {loss_plot_path}\")\n",
    "\n",
    "\n",
    "all_dates = df[nama_kolom_tanggal].values[FIXED_WINDOW_SIZE:]\n",
    "\n",
    "# Ambil tanggal khusus untuk bagian TEST saja\n",
    "test_dates = all_dates[train_size:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "plt.plot(test_dates, y_test_orig, label='Actual (Real Data)')\n",
    "plt.plot(test_dates, predictions_real, label='Predicted (Denormalized)')\n",
    "\n",
    "plt.title(f'{region_name} | W{FIXED_WINDOW_SIZE} E{FIXED_EPOCH} B{FIXED_BATCH_SIZE} DO{DROPOUT_RATE}')\n",
    "plt.xlabel('Tanggal dan Tahun')\n",
    "plt.ylabel('Harga (Rp)')\n",
    "\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) \n",
    "plt.gcf().autofmt_xdate() \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plot_path = os.path.join(region_out_dir, f\"{region_name}_Plot.png\")\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ✓ Plot disimpan: {plot_path}\")\n",
    "\n",
    "y_test_safe = np.where(y_test_orig.flatten() == 0, np.finfo(float).eps, y_test_orig.flatten())\n",
    "err_pct = np.abs((y_test_orig.flatten() - predictions_real.flatten()) / y_test_safe) * 100\n",
    "out_df = pd.DataFrame({\n",
    "    'Tanggal': test_dates,  \n",
    "    'Actual (Real)': y_test_orig.flatten(),\n",
    "    'Predicted (Real)': predictions_real.flatten(),\n",
    "    'Selisih': (y_test_orig.flatten() - predictions_real.flatten()),\n",
    "    'Error (%)': err_pct\n",
    "})\n",
    "excel_path = os.path.join(region_out_dir, f\"{region_name}_Prediksi_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "out_df.to_excel(excel_path, index=False)\n",
    "print(f\"   ✓ Prediksi disimpan: {excel_path}\")\n",
    "\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'Region': region_name,\n",
    "    'Window Size': int(FIXED_WINDOW_SIZE),\n",
    "    'Epoch': int(FIXED_EPOCH),\n",
    "    'Batch Size': int(FIXED_BATCH_SIZE),\n",
    "    'Dropout': float(DROPOUT_RATE),\n",
    "    'Best Epoch (val)': int(best_epoch),\n",
    "    'Best Val Loss': float(best_val),\n",
    "    'Last Loss': float(last_loss),\n",
    "    'Last Val Loss': float(last_val_loss) if last_val_loss is not None else None,\n",
    "    'MAPE (%)': float(mape)\n",
    "}])\n",
    "summary_path = os.path.join(region_out_dir, f\"{region_name}_Summary_W{FIXED_WINDOW_SIZE}_E{FIXED_EPOCH}_B{FIXED_BATCH_SIZE}.xlsx\")\n",
    "summary.to_excel(summary_path, index=False)\n",
    "print(f\"   ✓ Ringkasan disimpan: {summary_path}\")\n",
    "\n",
    "print(\"\\nSelesai untuk file ini.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67951f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
